{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f3831ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0932ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    branch=\"master\",\n",
    "    repo_path=\"./langchain\",\n",
    "    file_filter=file_filter\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865db9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "emb = AzureOpenAIEmbeddings(\n",
    "    model = \"text-embedding-ada-002\"\n",
    ")\n",
    "# AOAI の RateLimit で 429 を避けるため、ドキュメントの数を絞る\n",
    "db = Chroma.from_documents(documents[:10], emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddcf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    以下の文脈だけを参考にして、質問に答えてください。\n",
    "    文脈: {context}\n",
    "    質問: {question}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import ClientSecretCredential, get_bearer_token_provider\n",
    "\n",
    "token_provider = get_bearer_token_provider(\n",
    "    ClientSecretCredential(\n",
    "        client_id=os.getenv(\"client_id\"),\n",
    "        client_secret=os.getenv(\"client_secret\"),\n",
    "        tenant_id=os.getenv(\"tenant_id\")\n",
    "    ),\n",
    "    \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f31d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b10b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最初の {\"context\": retriever, \"question\": RunnablePassthrough()} は RunnableParallel が効いている\n",
    "# そのため、context と question は同時に処理される\n",
    "# chain.invoke の引数は並行して retriever と RunnablePassthrough に渡される\n",
    "chain = {\n",
    "    \"context\": retriever,\n",
    "    \"question\": RunnablePassthrough()\n",
    "} | prompt | llm | StrOutputParser()\n",
    "\n",
    "output = chain.invoke(\"LangChainについて教えてください。\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e667e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainは、大規模言語モデル（LLMs）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMを活用したアプリケーションのライフサイクル全体を簡素化することを目的としています。以下にLangChainの主な特徴と機能を説明します。\n",
      "\n",
      "---\n",
      "\n",
      "### **主な特徴**\n",
      "1. **開発（Development）**:\n",
      "   - LangChainは、オープンソースの[コンポーネント](/docs/concepts)や[サードパーティ統合](/docs/integrations/providers/)を利用してアプリケーションを構築することを支援します。\n",
      "   - [LangGraph](/docs/concepts/architecture/#langgraph)を使用して、ストリーミングや人間の介入をサポートする状態管理エージェントを構築できます。\n",
      "\n",
      "2. **プロダクション化（Productionization）**:\n",
      "   - [LangSmith](https://docs.smith.langchain.com/)を使用して、アプリケーションの検査、監視、評価を行い、継続的な最適化と信頼性のあるデプロイを実現します。\n",
      "\n",
      "3. **デプロイメント（Deployment）**:\n",
      "   - [LangGraph Platform](https://docs.langchain.com/langgraph-platform)を利用して、LangGraphアプリケーションをプロダクション対応のAPIやアシスタントに変換できます。\n",
      "\n",
      "---\n",
      "\n",
      "### **アーキテクチャ**\n",
      "LangChainは複数のオープンソースライブラリで構成されています：\n",
      "- **`langchain-core`**: チャットモデルやその他のコンポーネントの基本的な抽象化を提供。\n",
      "- **統合パッケージ**: 重要な統合（例: `langchain-openai`, `langchain-anthropic`）は軽量パッケージとして分割され、LangChainチームと統合開発者によって共同管理されています。\n",
      "- **`langchain`**: アプリケーションの認知アーキテクチャを構成するチェーン、エージェント、検索戦略を提供。\n",
      "- **`langchain-community`**: コミュニティによって管理されるサードパーティ統合。\n",
      "- **`langgraph`**: LangChainコンポーネントを組み合わせて永続性やストリーミングなどの機能を備えたプロダクション対応アプリケーションを構築するためのオーケストレーションフレームワーク。\n",
      "\n",
      "---\n",
      "\n",
      "### **ガイドとリソース**\n",
      "1. **チュートリアル**:\n",
      "   - [シンプルなLLMアプリケーションの構築](/docs/tutorials/llm_chain)\n",
      "   - [チャットボットの構築](/docs/tutorials/chatbot)\n",
      "   - [エージェントの構築](/docs/tutorials/agents)\n",
      "   - [LangGraphの紹介](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n",
      "\n",
      "2. **ハウツーガイド**:\n",
      "   - 「どうやって〇〇をするのか？」という質問に短い回答を提供。\n",
      "   - [ハウツーガイドはこちら](/docs/how_to)。\n",
      "\n",
      "3. **概念ガイド**:\n",
      "   - LangChainの主要な概念を高レベルで説明。\n",
      "   - [概念ガイドはこちら](/docs/concepts)。\n",
      "\n",
      "4. **統合**:\n",
      "   - LangChainは、チャットモデルやベクトルストアなど、さまざまなプロバイダーと統合可能。\n",
      "   - [統合の詳細はこちら](/docs/integrations/providers/)。\n",
      "\n",
      "5. **APIリファレンス**:\n",
      "   - LangChain Pythonパッケージのクラスやメソッドの完全なドキュメントを提供。\n",
      "   - [APIリファレンスはこちら](https://python.langchain.com/api_reference/)。\n",
      "\n",
      "---\n",
      "\n",
      "### **エコシステム**\n",
      "- **LangSmith**: アプリケーションのトレースと評価を行い、プロトタイプからプロダクションへの移行を支援。\n",
      "- **LangGraph**: LLMを活用した状態管理型のマルチアクターアプリケーションを構築。\n",
      "\n",
      "---\n",
      "\n",
      "LangChainは、LLMを活用したアプリケーションの開発、最適化、デプロイを効率化するための強力なツールセットを提供します。\n"
     ]
    }
   ],
   "source": [
    "chain = {\n",
    "    \"context\": retriever,\n",
    "    \"question\": RunnablePassthrough()\n",
    "} | RunnablePassthrough.assign( ans = prompt | llm | StrOutputParser())\n",
    "\n",
    "output = chain.invoke(\"LangChainについて教えてください。\")\n",
    "print(output['ans'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff16a70",
   "metadata": {},
   "source": [
    "## HyDE というテクニック\n",
    "確かに、RAGの仕組み上疑問ではあったが、なぜ質問に対して類似する文章をベクトルDBから探すんだろうと思っていた。\n",
    "探しているのは回答なので、一度LLMに回答を出力させてからそれに類似するチャンクをDBから引っ張ってくるほうが分布としては近いんじゃないか？というのがHyDE。\n",
    "つまり、ベクトルDBに対する検索クエリの工夫によって精度向上を目指すという話ね。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    次の質問に回答する一文を書いてください。\n",
    "    質問: {question}\n",
    "    '''\n",
    ")\n",
    "hypo_chain = hypothetical_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a9e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainは、言語モデルを活用したアプリケーション開発を容易にするためのフレームワークであり、データ接続、対話型エージェント、チェーン構築などの機能を提供します。\n"
     ]
    }
   ],
   "source": [
    "# 本来ユーザからの質問が直接 retriever に渡るのを、hypo_chain の出力を渡すようにする\n",
    "hyde_rag_chain = {\n",
    "    \"context\": hypo_chain | retriever,\n",
    "    \"question\": RunnablePassthrough()\n",
    "} | RunnablePassthrough.assign( ans = hypothetical_prompt | llm | StrOutputParser())\n",
    "output = hyde_rag_chain.invoke(\"LangChainについて教えてください。\")\n",
    "print(output['ans'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93787b16",
   "metadata": {},
   "source": [
    "## ユーザのクエリの違うパターンを LLM に生成させる\n",
    "これにより、類似の質問が複数渡されるため、BootStrapのようなイメージで少し異なるコンテキストに対する retrieve 結果をもとに回答できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd058161",
   "metadata": {},
   "source": [
    "## 複数クエリを与える場合のチャンクの重みづけ\n",
    "複数クエリのそれぞれで共通して参照されるチャンクは重みをつけることで、なるべくそのチャンクを重視した最終回答を生成させる。やり方としては、RRF値による並べ替え、並べ替え＋上位ｎ位以下の切り捨て、など"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833adf45",
   "metadata": {},
   "source": [
    "## 単一クエリの場合の参照チャンクのリランク\n",
    "ユーザのクエリと近いベクトルを持つチャンクを複数引っ張ってきたのち、そのチャンクをリランクタスクにおいて精度の高い別のモデルによって再度並べ替えを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='218069c7-6e2b-4733-821a-ba3579b9dcf1', metadata={'file_name': 'introduction.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\introduction.mdx', 'file_path': 'docs\\\\docs\\\\introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://docs.langchain.com/langgraph-platform).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'), Document(id='6be94cdb-ef67-4a02-936b-9cce6e8667df', metadata={'source': 'docs\\\\docs\\\\additional_resources\\\\arxiv_references.mdx', 'file_type': '.mdx', 'file_name': 'arxiv_references.mdx', 'file_path': 'docs\\\\docs\\\\additional_resources\\\\arxiv_references.mdx'}, page_content='# arXiv\\n\\nLangChain implements the latest research in the field of Natural Language Processing.\\nThis page contains `arXiv` papers referenced in the LangChain Documentation, API Reference,\\n Templates, and Cookbooks.\\n\\nFrom the opposite direction, scientists use `LangChain` in research and reference it in the research papers.\\n\\n`arXiv` papers with references to:\\n [LangChain](https://arxiv.org/search/?query=langchain&searchtype=all&source=header) | [LangGraph](https://arxiv.org/search/?query=langgraph&searchtype=all&source=header) | [LangSmith](https://arxiv.org/search/?query=langsmith&searchtype=all&source=header)\\n\\n## Summary\\n\\n| arXiv id / Title | Authors | Published date 🔻 | LangChain Documentation|\\n|------------------|---------|-------------------|------------------------|\\n| `2403.14403v2` [Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity](http://arxiv.org/abs/2403.14403v2) | Soyeong Jeong, Jinheon Baek, Sukmin Cho,  et al. | 2024&#8209;03&#8209;21 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2402.03620v1` [Self-Discover: Large Language Models Self-Compose Reasoning Structures](http://arxiv.org/abs/2402.03620v1) | Pei Zhou, Jay Pujara, Xiang Ren,  et al. | 2024&#8209;02&#8209;06 | `Cookbook:` [Self-Discover](https://github.com/langchain-ai/langchain/blob/master/cookbook/self-discover.ipynb)\\n| `2402.03367v2` [RAG-Fusion: a New Take on Retrieval-Augmented Generation](http://arxiv.org/abs/2402.03367v2) | Zackary Rackauckas | 2024&#8209;01&#8209;31 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2401.18059v1` [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](http://arxiv.org/abs/2401.18059v1) | Parth Sarthi, Salman Abdullah, Aditi Tuli,  et al. | 2024&#8209;01&#8209;31 | `Cookbook:` [Raptor](https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb)\\n| `2401.15884v2` [Corrective Retrieval Augmented Generation](http://arxiv.org/abs/2401.15884v2) | Shi-Qi Yan, Jia-Chen Gu, Yun Zhu,  et al. | 2024&#8209;01&#8209;29 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts), `Cookbook:` [Langgraph Crag](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_crag.ipynb)\\n| `2401.08500v1` [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](http://arxiv.org/abs/2401.08500v1) | Tal Ridnik, Dedy Kredo, Itamar Friedman | 2024&#8209;01&#8209;16 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2401.04088v1` [Mixtral of Experts](http://arxiv.org/abs/2401.04088v1) | Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,  et al. | 2024&#8209;01&#8209;08 | `Cookbook:` [Together Ai](https://github.com/langchain-ai/langchain/blob/master/cookbook/together_ai.ipynb)\\n| `2312.06648v2` [Dense X Retrieval: What Retrieval Granularity Should We Use?](http://arxiv.org/abs/2312.06648v2) | Tong Chen, Hongwei Wang, Sihao Chen,  et al. | 2023&#8209;12&#8209;11 | `Template:` [propositional-retrieval](https://python.langchain.com/docs/templates/propositional-retrieval)\\n| `2311.09210v1` [Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models](http://arxiv.org/abs/2311.09210v1) | Wenhao Yu, Hongming Zhang, Xiaoman Pan,  et al. | 2023&#8209;11&#8209;15 | `Template:` [chain-of-note-wiki](https://python.langchain.com/docs/templates/chain-of-note-wiki)\\n| `2310.11511v1` [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](http://arxiv.org/abs/2310.11511v1) | Akari Asai, Zeqiu Wu, Yizhong Wang,  et al. | 2023&#8209;10&#8209;17 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts), `Cookbook:` [Langgraph Self Rag](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_self_rag.ipynb)\\n| `2310.06117v2` [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](http://arxiv.org/abs/2310.06117v2) | Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,  et al. | 2023&#8209;10&#8209;09 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts), `Template:` [stepback-qa-prompting](https://python.langchain.com/docs/templates/stepback-qa-prompting), `Cookbook:` [Stepback-Qa](https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb)\\n| `2307.15337v3` [Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation](http://arxiv.org/abs/2307.15337v3) | Xuefei Ning, Zinan Lin, Zixuan Zhou,  et al. | 2023&#8209;07&#8209;28 | `Template:` [skeleton-of-thought](https://python.langchain.com/docs/templates/skeleton-of-thought)\\n| `2307.09288v2` [Llama 2: Open Foundation and Fine-Tuned Chat Models](http://arxiv.org/abs/2307.09288v2) | Hugo Touvron, Louis Martin, Kevin Stone,  et al. | 2023&#8209;07&#8209;18 | `Cookbook:` [Semi Structured Rag](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb)\\n| `2307.03172v3` [Lost in the Middle: How Language Models Use Long Contexts](http://arxiv.org/abs/2307.03172v3) | Nelson F. Liu, Kevin Lin, John Hewitt,  et al. | 2023&#8209;07&#8209;06 | `Docs:` [docs/how_to/long_context_reorder](https://python.langchain.com/docs/how_to/long_context_reorder)\\n| `2305.14283v3` [Query Rewriting for Retrieval-Augmented Large Language Models](http://arxiv.org/abs/2305.14283v3) | Xinbei Ma, Yeyun Gong, Pengcheng He,  et al. | 2023&#8209;05&#8209;23 | `Template:` [rewrite-retrieve-read](https://python.langchain.com/docs/templates/rewrite-retrieve-read), `Cookbook:` [Rewrite](https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb)\\n| `2305.08291v1` [Large Language Model Guided Tree-of-Thought](http://arxiv.org/abs/2305.08291v1) | Jieyi Long | 2023&#8209;05&#8209;15 | `API:` [langchain_experimental.tot](https://python.langchain.com/api_reference/experimental/tot.html), `Cookbook:` [Tree Of Thought](https://github.com/langchain-ai/langchain/blob/master/cookbook/tree_of_thought.ipynb)\\n| `2305.04091v3` [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](http://arxiv.org/abs/2305.04091v3) | Lei Wang, Wanyu Xu, Yihuai Lan,  et al. | 2023&#8209;05&#8209;06 | `Cookbook:` [Plan And Execute Agent](https://github.com/langchain-ai/langchain/blob/master/cookbook/plan_and_execute_agent.ipynb)\\n| `2305.02156v1` [Zero-Shot Listwise Document Reranking with a Large Language Model](http://arxiv.org/abs/2305.02156v1) | Xueguang Ma, Xinyu Zhang, Ronak Pradeep,  et al. | 2023&#8209;05&#8209;03 | `Docs:` [docs/how_to/contextual_compression](https://python.langchain.com/docs/how_to/contextual_compression), `API:` [langchain...LLMListwiseRerank](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank.html#)\\n| `2304.08485v2` [Visual Instruction Tuning](http://arxiv.org/abs/2304.08485v2) | Haotian Liu, Chunyuan Li, Qingyang Wu,  et al. | 2023&#8209;04&#8209;17 | `Cookbook:` [Semi Structured Multi Modal Rag Llama2](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb), [Semi Structured And Multi Modal Rag](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb)\\n| `2304.03442v2` [Generative Agents: Interactive Simulacra of Human Behavior](http://arxiv.org/abs/2304.03442v2) | Joon Sung Park, Joseph C. O\\'Brien, Carrie J. Cai,  et al. | 2023&#8209;04&#8209;07 | `Cookbook:` [Generative Agents Interactive Simulacra Of Human Behavior](https://github.com/langchain-ai/langchain/blob/master/cookbook/generative_agents_interactive_simulacra_of_human_behavior.ipynb), [Multiagent Bidding](https://github.com/langchain-ai/langchain/blob/master/cookbook/multiagent_bidding.ipynb)\\n| `2303.17760v2` [CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society](http://arxiv.org/abs/2303.17760v2) | Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani,  et al. | 2023&#8209;03&#8209;31 | `Cookbook:` [Camel Role Playing](https://github.com/langchain-ai/langchain/blob/master/cookbook/camel_role_playing.ipynb)\\n| `2303.17580v4` [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](http://arxiv.org/abs/2303.17580v4) | Yongliang Shen, Kaitao Song, Xu Tan,  et al. | 2023&#8209;03&#8209;30 | `API:` [langchain_experimental.autonomous_agents](https://python.langchain.com/api_reference/experimental/autonomous_agents.html), `Cookbook:` [Hugginggpt](https://github.com/langchain-ai/langchain/blob/master/cookbook/hugginggpt.ipynb)\\n| `2301.10226v4` [A Watermark for Large Language Models](http://arxiv.org/abs/2301.10226v4) | John Kirchenbauer, Jonas Geiping, Yuxin Wen,  et al. | 2023&#8209;01&#8209;24 | `API:` [langchain_community...OCIModelDeploymentTGI](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI.html#langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI), [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)\\n| `2212.10496v1` [Precise Zero-Shot Dense Retrieval without Relevance Labels](http://arxiv.org/abs/2212.10496v1) | Luyu Gao, Xueguang Ma, Jimmy Lin,  et al. | 2022&#8209;12&#8209;20 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts), `API:` [langchain...HypotheticalDocumentEmbedder](https://api.python.langchain.com/en/latest/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html#langchain.chains.hyde.base.HypotheticalDocumentEmbedder), `Template:` [hyde](https://python.langchain.com/docs/templates/hyde), `Cookbook:` [Hypothetical Document Embeddings](https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb)\\n| `2212.08073v1` [Constitutional AI: Harmlessness from AI Feedback](http://arxiv.org/abs/2212.08073v1) | Yuntao Bai, Saurav Kadavath, Sandipan Kundu,  et al. | 2022&#8209;12&#8209;15 | `Docs:` [docs/versions/migrating_chains/constitutional_chain](https://python.langchain.com/docs/versions/migrating_chains/constitutional_chain)\\n| `2212.07425v3` [Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments](http://arxiv.org/abs/2212.07425v3) | Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande,  et al. | 2022&#8209;12&#8209;12 | `API:` [langchain_experimental.fallacy_removal](https://python.langchain.com/api_reference/experimental/fallacy_removal.html)\\n| `2211.13892v2` [Complementary Explanations for Effective In-Context Learning](http://arxiv.org/abs/2211.13892v2) | Xi Ye, Srinivasan Iyer, Asli Celikyilmaz,  et al. | 2022&#8209;11&#8209;25 | `API:` [langchain_core...MaxMarginalRelevanceExampleSelector](https://api.python.langchain.com/en/latest/example_selectors/langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector.html#langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector)\\n| `2211.10435v2` [PAL: Program-aided Language Models](http://arxiv.org/abs/2211.10435v2) | Luyu Gao, Aman Madaan, Shuyan Zhou,  et al. | 2022&#8209;11&#8209;18 | `API:` [langchain_experimental.pal_chain](https://python.langchain.com/api_reference/experimental/pal_chain.html), [langchain_experimental...PALChain](https://api.python.langchain.com/en/latest/pal_chain/langchain_experimental.pal_chain.base.PALChain.html#langchain_experimental.pal_chain.base.PALChain), `Cookbook:` [Program Aided Language Model](https://github.com/langchain-ai/langchain/blob/master/cookbook/program_aided_language_model.ipynb)\\n| `2210.11934v2` [An Analysis of Fusion Functions for Hybrid Retrieval](http://arxiv.org/abs/2210.11934v2) | Sebastian Bruch, Siyu Gai, Amir Ingber | 2022&#8209;10&#8209;21 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2210.03629v3` [ReAct: Synergizing Reasoning and Acting in Language Models](http://arxiv.org/abs/2210.03629v3) | Shunyu Yao, Jeffrey Zhao, Dian Yu,  et al. | 2022&#8209;10&#8209;06 | `Docs:` [docs/integrations/tools/ionic_shopping](https://python.langchain.com/docs/integrations/tools/ionic_shopping), [docs/integrations/providers/cohere](https://python.langchain.com/docs/integrations/providers/cohere), [docs/concepts](https://python.langchain.com/docs/concepts), `API:` [langchain...create_react_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.react.agent.create_react_agent.html#langchain.agents.react.agent.create_react_agent), [langchain...TrajectoryEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain)\\n| `2209.10785v2` [Deep Lake: a Lakehouse for Deep Learning](http://arxiv.org/abs/2209.10785v2) | Sasun Hambardzumyan, Abhinav Tuli, Levon Ghukasyan,  et al. | 2022&#8209;09&#8209;22 | `Docs:` [docs/integrations/providers/activeloop_deeplake](https://python.langchain.com/docs/integrations/providers/activeloop_deeplake)\\n| `2205.13147v4` [Matryoshka Representation Learning](http://arxiv.org/abs/2205.13147v4) | Aditya Kusupati, Gantavya Bhatt, Aniket Rege,  et al. | 2022&#8209;05&#8209;26 | `Docs:` [docs/integrations/providers/snowflake](https://python.langchain.com/docs/integrations/providers/snowflake)\\n| `2205.12654v1` [Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages](http://arxiv.org/abs/2205.12654v1) | Kevin Heffernan, Onur Çelebi, Holger Schwenk | 2022&#8209;05&#8209;25 | `API:` [langchain_community...LaserEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.laser.LaserEmbeddings.html#langchain_community.embeddings.laser.LaserEmbeddings)\\n| `2204.00498v1` [Evaluating the Text-to-SQL Capabilities of Large Language Models](http://arxiv.org/abs/2204.00498v1) | Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau | 2022&#8209;03&#8209;15 | `Docs:` [docs/tutorials/sql_qa](https://python.langchain.com/docs/tutorials/sql_qa), `API:` [langchain_community...SQLDatabase](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.sql_database.SQLDatabase.html#langchain_community.utilities.sql_database.SQLDatabase), [langchain_community...SparkSQL](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.spark_sql.SparkSQL.html#langchain_community.utilities.spark_sql.SparkSQL)\\n| `2202.00666v5` [Locally Typical Sampling](http://arxiv.org/abs/2202.00666v5) | Clara Meister, Tiago Pimentel, Gian Wiher,  et al. | 2022&#8209;02&#8209;01 | `API:` [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)\\n| `2112.01488v3` [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](http://arxiv.org/abs/2112.01488v3) | Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,  et al. | 2021&#8209;12&#8209;02 | `Docs:` [docs/integrations/retrievers/ragatouille](https://python.langchain.com/docs/integrations/retrievers/ragatouille), [docs/integrations/providers/ragatouille](https://python.langchain.com/docs/integrations/providers/ragatouille), [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2103.00020v1` [Learning Transferable Visual Models From Natural Language Supervision](http://arxiv.org/abs/2103.00020v1) | Alec Radford, Jong Wook Kim, Chris Hallacy,  et al. | 2021&#8209;02&#8209;26 | `API:` [langchain_experimental.open_clip](https://python.langchain.com/api_reference/experimental/open_clip.html)\\n| `2005.14165v4` [Language Models are Few-Shot Learners](http://arxiv.org/abs/2005.14165v4) | Tom B. Brown, Benjamin Mann, Nick Ryder,  et al. | 2020&#8209;05&#8209;28 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2005.11401v4` [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](http://arxiv.org/abs/2005.11401v4) | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020&#8209;05&#8209;22 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `1909.05858v2` [CTRL: A Conditional Transformer Language Model for Controllable Generation](http://arxiv.org/abs/1909.05858v2) | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019&#8209;09&#8209;11 | `API:` [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)\\n\\n## Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity\\n\\n- **Authors:** Soyeong Jeong, Jinheon Baek, Sukmin Cho,  et al.\\n- **arXiv id:** [2403.14403v2](http://arxiv.org/abs/2403.14403v2)  **Published Date:** 2024-03-21\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n\\n**Abstract:** Retrieval-Augmented Large Language Models (LLMs), which incorporate the\\nnon-parametric knowledge from external knowledge bases into LLMs, have emerged\\nas a promising approach to enhancing response accuracy in several tasks, such\\nas Question-Answering (QA). However, even though there are various approaches\\ndealing with queries of different complexities, they either handle simple\\nqueries with unnecessary computational overhead or fail to adequately address\\ncomplex multi-step queries; yet, not all user requests fall into only one of\\nthe simple or complex categories. In this work, we propose a novel adaptive QA\\nframework, that can dynamically select the most suitable strategy for\\n(retrieval-augmented) LLMs from the simplest to the most sophisticated ones\\nbased on the query complexity. Also, this selection process is operationalized\\nwith a classifier, which is a smaller LM trained to predict the complexity\\nlevel of incoming queries with automatically collected labels, obtained from\\nactual predicted outcomes of models and inherent inductive biases in datasets.\\nThis approach offers a balanced strategy, seamlessly adapting between the\\niterative and single-step retrieval-augmented LLMs, as well as the no-retrieval\\nmethods, in response to a range of query complexities. We validate our model on\\na set of open-domain QA datasets, covering multiple query complexities, and\\nshow that ours enhances the overall efficiency and accuracy of QA systems,\\ncompared to relevant baselines including the adaptive retrieval approaches.\\nCode is available at: https://github.com/starsuzi/Adaptive-RAG.\\n\\n## Self-Discover: Large Language Models Self-Compose Reasoning Structures\\n\\n- **Authors:** Pei Zhou, Jay Pujara, Xiang Ren,  et al.\\n- **arXiv id:** [2402.03620v1](http://arxiv.org/abs/2402.03620v1)  **Published Date:** 2024-02-06\\n- **LangChain:**\\n\\n   - **Cookbook:** [self-discover](https://github.com/langchain-ai/langchain/blob/master/cookbook/self-discover.ipynb)\\n\\n**Abstract:** We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the\\ntask-intrinsic reasoning structures to tackle complex reasoning problems that\\nare challenging for typical prompting methods. Core to the framework is a\\nself-discovery process where LLMs select multiple atomic reasoning modules such\\nas critical thinking and step-by-step thinking, and compose them into an\\nexplicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER\\nsubstantially improves GPT-4 and PaLM 2\\'s performance on challenging reasoning\\nbenchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as\\nmuch as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER\\noutperforms inference-intensive methods such as CoT-Self-Consistency by more\\nthan 20%, while requiring 10-40x fewer inference compute. Finally, we show that\\nthe self-discovered reasoning structures are universally applicable across\\nmodel families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share\\ncommonalities with human reasoning patterns.\\n\\n## RAG-Fusion: a New Take on Retrieval-Augmented Generation\\n\\n- **Authors:** Zackary Rackauckas\\n- **arXiv id:** [2402.03367v2](http://arxiv.org/abs/2402.03367v2)  **Published Date:** 2024-01-31\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n\\n**Abstract:** Infineon has identified a need for engineers, account managers, and customers\\nto rapidly obtain product information. This problem is traditionally addressed\\nwith retrieval-augmented generation (RAG) chatbots, but in this study, I\\nevaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion\\ncombines RAG and reciprocal rank fusion (RRF) by generating multiple queries,\\nreranking them with reciprocal scores and fusing the documents and scores.\\nThrough manually evaluating answers on accuracy, relevance, and\\ncomprehensiveness, I found that RAG-Fusion was able to provide accurate and\\ncomprehensive answers due to the generated queries contextualizing the original\\nquery from various perspectives. However, some answers strayed off topic when\\nthe generated queries\\' relevance to the original query is insufficient. This\\nresearch marks significant progress in artificial intelligence (AI) and natural\\nlanguage processing (NLP) applications and demonstrates transformations in a\\nglobal and multi-industry context.\\n\\n## RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\\n\\n- **Authors:** Parth Sarthi, Salman Abdullah, Aditi Tuli,  et al.\\n- **arXiv id:** [2401.18059v1](http://arxiv.org/abs/2401.18059v1)  **Published Date:** 2024-01-31\\n- **LangChain:**\\n\\n   - **Cookbook:** [RAPTOR](https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb)\\n\\n**Abstract:** Retrieval-augmented language models can better adapt to changes in world\\nstate and incorporate long-tail knowledge. However, most existing methods\\nretrieve only short contiguous chunks from a retrieval corpus, limiting\\nholistic understanding of the overall document context. We introduce the novel\\napproach of recursively embedding, clustering, and summarizing chunks of text,\\nconstructing a tree with differing levels of summarization from the bottom up.\\nAt inference time, our RAPTOR model retrieves from this tree, integrating\\ninformation across lengthy documents at different levels of abstraction.\\nControlled experiments show that retrieval with recursive summaries offers\\nsignificant improvements over traditional retrieval-augmented LMs on several\\ntasks. On question-answering tasks that involve complex, multi-step reasoning,\\nwe show state-of-the-art results; for example, by coupling RAPTOR retrieval\\nwith the use of GPT-4, we can improve the best performance on the QuALITY\\nbenchmark by 20% in absolute accuracy.\\n\\n## Corrective Retrieval Augmented Generation\\n\\n- **Authors:** Shi-Qi Yan, Jia-Chen Gu, Yun Zhu,  et al.\\n- **arXiv id:** [2401.15884v2](http://arxiv.org/abs/2401.15884v2)  **Published Date:** 2024-01-29\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **Cookbook:** [langgraph_crag](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_crag.ipynb)\\n\\n**Abstract:** Large language models (LLMs) inevitably exhibit hallucinations since the\\naccuracy of generated texts cannot be secured solely by the parametric\\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\\ndocuments, raising concerns about how the model behaves if retrieval goes\\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\\nretrieval evaluator is designed to assess the overall quality of retrieved\\ndocuments for a query, returning a confidence degree based on which different\\nknowledge retrieval actions can be triggered. Since retrieval from static and\\nlimited corpora can only return sub-optimal documents, large-scale web searches\\nare utilized as an extension for augmenting the retrieval results. Besides, a\\ndecompose-then-recompose algorithm is designed for retrieved documents to\\nselectively focus on key information and filter out irrelevant information in\\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\\nRAG-based approaches. Experiments on four datasets covering short- and\\nlong-form generation tasks show that CRAG can significantly improve the\\nperformance of RAG-based approaches.\\n\\n## Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\\n\\n- **Authors:** Tal Ridnik, Dedy Kredo, Itamar Friedman\\n- **arXiv id:** [2401.08500v1](http://arxiv.org/abs/2401.08500v1)  **Published Date:** 2024-01-16\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n\\n**Abstract:** Code generation problems differ from common natural language problems - they\\nrequire matching the exact syntax of the target language, identifying happy\\npaths and edge cases, paying attention to numerous small details in the problem\\nspec, and addressing other code-specific issues and requirements. Hence, many\\nof the optimizations and tricks that have been successful in natural language\\ngeneration may not be effective for code tasks. In this work, we propose a new\\napproach to code generation by LLMs, which we call AlphaCodium - a test-based,\\nmulti-stage, code-oriented iterative flow, that improves the performances of\\nLLMs on code problems. We tested AlphaCodium on a challenging code generation\\ndataset called CodeContests, which includes competitive programming problems\\nfrom platforms such as Codeforces. The proposed flow consistently and\\nsignificantly improves results. On the validation set, for example, GPT-4\\naccuracy (pass@5) increased from 19% with a single well-designed direct prompt\\nto 44% with the AlphaCodium flow. Many of the principles and best practices\\nacquired in this work, we believe, are broadly applicable to general code\\ngeneration tasks. Full implementation is available at:\\nhttps://github.com/Codium-ai/AlphaCodium\\n\\n## Mixtral of Experts\\n\\n- **Authors:** Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,  et al.\\n- **arXiv id:** [2401.04088v1](http://arxiv.org/abs/2401.04088v1)  **Published Date:** 2024-01-08\\n- **LangChain:**\\n\\n   - **Cookbook:** [together_ai](https://github.com/langchain-ai/langchain/blob/master/cookbook/together_ai.ipynb)\\n\\n**Abstract:** We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\\nMixtral has the same architecture as Mistral 7B, with the difference that each\\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\\neach layer, a router network selects two experts to process the current state\\nand combine their outputs. Even though each token only sees two experts, the\\nselected experts can be different at each timestep. As a result, each token has\\naccess to 47B parameters, but only uses 13B active parameters during inference.\\nMixtral was trained with a context size of 32k tokens and it outperforms or\\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\\nmultilingual benchmarks. We also provide a model fine-tuned to follow\\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\\nthe base and instruct models are released under the Apache 2.0 license.\\n\\n## Dense X Retrieval: What Retrieval Granularity Should We Use?\\n\\n- **Authors:** Tong Chen, Hongwei Wang, Sihao Chen,  et al.\\n- **arXiv id:** [2312.06648v2](http://arxiv.org/abs/2312.06648v2)  **Published Date:** 2023-12-11\\n- **LangChain:**\\n\\n   - **Template:** [propositional-retrieval](https://python.langchain.com/docs/templates/propositional-retrieval)\\n\\n**Abstract:** Dense retrieval has become a prominent method to obtain relevant context or\\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\\non a retrieval corpus at inference time, an often-overlooked design choice is\\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\\nsentence. We discover that the retrieval unit choice significantly impacts the\\nperformance of both retrieval and downstream tasks. Distinct from the typical\\napproach of using passages or sentences, we introduce a novel retrieval unit,\\nproposition, for dense retrieval. Propositions are defined as atomic\\nexpressions within text, each encapsulating a distinct factoid and presented in\\na concise, self-contained natural language format. We conduct an empirical\\ncomparison of different retrieval granularity. Our results reveal that\\nproposition-based retrieval significantly outperforms traditional passage or\\nsentence-based methods in dense retrieval. Moreover, retrieval by proposition\\nalso enhances the performance of downstream QA tasks, since the retrieved texts\\nare more condensed with question-relevant information, reducing the need for\\nlengthy input tokens and minimizing the inclusion of extraneous, irrelevant\\ninformation.\\n\\n## Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models\\n\\n- **Authors:** Wenhao Yu, Hongming Zhang, Xiaoman Pan,  et al.\\n- **arXiv id:** [2311.09210v1](http://arxiv.org/abs/2311.09210v1)  **Published Date:** 2023-11-15\\n- **LangChain:**\\n\\n   - **Template:** [chain-of-note-wiki](https://python.langchain.com/docs/templates/chain-of-note-wiki)\\n\\n**Abstract:** Retrieval-augmented language models (RALMs) represent a substantial\\nadvancement in the capabilities of large language models, notably in reducing\\nfactual hallucination by leveraging external knowledge sources. However, the\\nreliability of the retrieved information is not always guaranteed. The\\nretrieval of irrelevant data can lead to misguided responses, and potentially\\ncausing the model to overlook its inherent knowledge, even when it possesses\\nadequate information to address the query. Moreover, standard RALMs often\\nstruggle to assess whether they possess adequate knowledge, both intrinsic and\\nretrieved, to provide an accurate answer. In situations where knowledge is\\nlacking, these systems should ideally respond with \"unknown\" when the answer is\\nunattainable. In response to these challenges, we introduces Chain-of-Noting\\n(CoN), a novel approach aimed at improving the robustness of RALMs in facing\\nnoisy, irrelevant documents and in handling unknown scenarios. The core idea of\\nCoN is to generate sequential reading notes for retrieved documents, enabling a\\nthorough evaluation of their relevance to the given question and integrating\\nthis information to formulate the final answer. We employed ChatGPT to create\\ntraining data for CoN, which was subsequently trained on an LLaMa-2 7B model.\\nOur experiments across four open-domain QA benchmarks show that RALMs equipped\\nwith CoN significantly outperform standard RALMs. Notably, CoN achieves an\\naverage improvement of +7.9 in EM score given entirely noisy retrieved\\ndocuments and +10.5 in rejection rates for real-time questions that fall\\noutside the pre-training knowledge scope.\\n\\n## Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\\n\\n- **Authors:** Akari Asai, Zeqiu Wu, Yizhong Wang,  et al.\\n- **arXiv id:** [2310.11511v1](http://arxiv.org/abs/2310.11511v1)  **Published Date:** 2023-10-17\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **Cookbook:** [langgraph_self_rag](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_self_rag.ipynb)\\n\\n**Abstract:** Despite their remarkable capabilities, large language models (LLMs) often\\nproduce responses containing factual inaccuracies due to their sole reliance on\\nthe parametric knowledge they encapsulate. Retrieval-Augmented Generation\\n(RAG), an ad hoc approach that augments LMs with retrieval of relevant\\nknowledge, decreases such issues. However, indiscriminately retrieving and\\nincorporating a fixed number of retrieved passages, regardless of whether\\nretrieval is necessary, or passages are relevant, diminishes LM versatility or\\ncan lead to unhelpful response generation. We introduce a new framework called\\nSelf-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM\\'s\\nquality and factuality through retrieval and self-reflection. Our framework\\ntrains a single arbitrary LM that adaptively retrieves passages on-demand, and\\ngenerates and reflects on retrieved passages and its own generations using\\nspecial tokens, called reflection tokens. Generating reflection tokens makes\\nthe LM controllable during the inference phase, enabling it to tailor its\\nbehavior to diverse task requirements. Experiments show that Self-RAG (7B and\\n13B parameters) significantly outperforms state-of-the-art LLMs and\\nretrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in\\nimproving factuality and citation accuracy for long-form generations relative\\nto these models.\\n\\n## Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\\n\\n- **Authors:** Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,  et al.\\n- **arXiv id:** [2310.06117v2](http://arxiv.org/abs/2310.06117v2)  **Published Date:** 2023-10-09\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **Template:** [stepback-qa-prompting](https://python.langchain.com/docs/templates/stepback-qa-prompting)\\n   - **Cookbook:** [stepback-qa](https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb)\\n\\n**Abstract:** We present Step-Back Prompting, a simple prompting technique that enables\\nLLMs to do abstractions to derive high-level concepts and first principles from\\ninstances containing specific details. Using the concepts and principles to\\nguide reasoning, LLMs significantly improve their abilities in following a\\ncorrect reasoning path towards the solution. We conduct experiments of\\nStep-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe\\nsubstantial performance gains on various challenging reasoning-intensive tasks\\nincluding STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back\\nPrompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%\\nand 11% respectively, TimeQA by 27%, and MuSiQue by 7%.\\n\\n## Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation\\n\\n- **Authors:** Xuefei Ning, Zinan Lin, Zixuan Zhou,  et al.\\n- **arXiv id:** [2307.15337v3](http://arxiv.org/abs/2307.15337v3)  **Published Date:** 2023-07-28\\n- **LangChain:**\\n\\n   - **Template:** [skeleton-of-thought](https://python.langchain.com/docs/templates/skeleton-of-thought)\\n\\n**Abstract:** This work aims at decreasing the end-to-end generation latency of large\\nlanguage models (LLMs). One of the major causes of the high generation latency\\nis the sequential decoding approach adopted by almost all state-of-the-art\\nLLMs. In this work, motivated by the thinking and writing process of humans, we\\npropose Skeleton-of-Thought (SoT), which first guides LLMs to generate the\\nskeleton of the answer, and then conducts parallel API calls or batched\\ndecoding to complete the contents of each skeleton point in parallel. Not only\\ndoes SoT provide considerable speed-ups across 12 LLMs, but it can also\\npotentially improve the answer quality on several question categories. SoT is\\nan initial attempt at data-centric optimization for inference efficiency, and\\nshowcases the potential of eliciting high-quality answers by explicitly\\nplanning the answer structure in language.\\n\\n## Llama 2: Open Foundation and Fine-Tuned Chat Models\\n\\n- **Authors:** Hugo Touvron, Louis Martin, Kevin Stone,  et al.\\n- **arXiv id:** [2307.09288v2](http://arxiv.org/abs/2307.09288v2)  **Published Date:** 2023-07-18\\n- **LangChain:**\\n\\n   - **Cookbook:** [Semi_Structured_RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb)\\n\\n**Abstract:** In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n\\n## Lost in the Middle: How Language Models Use Long Contexts\\n\\n- **Authors:** Nelson F. Liu, Kevin Lin, John Hewitt,  et al.\\n- **arXiv id:** [2307.03172v3](http://arxiv.org/abs/2307.03172v3)  **Published Date:** 2023-07-06\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/how_to/long_context_reorder](https://python.langchain.com/docs/how_to/long_context_reorder)\\n\\n**Abstract:** While recent language models have the ability to take long contexts as input,\\nrelatively little is known about how well they use longer context. We analyze\\nthe performance of language models on two tasks that require identifying\\nrelevant information in their input contexts: multi-document question answering\\nand key-value retrieval. We find that performance can degrade significantly\\nwhen changing the position of relevant information, indicating that current\\nlanguage models do not robustly make use of information in long input contexts.\\nIn particular, we observe that performance is often highest when relevant\\ninformation occurs at the beginning or end of the input context, and\\nsignificantly degrades when models must access relevant information in the\\nmiddle of long contexts, even for explicitly long-context models. Our analysis\\nprovides a better understanding of how language models use their input context\\nand provides new evaluation protocols for future long-context language models.\\n\\n## Query Rewriting for Retrieval-Augmented Large Language Models\\n\\n- **Authors:** Xinbei Ma, Yeyun Gong, Pengcheng He,  et al.\\n- **arXiv id:** [2305.14283v3](http://arxiv.org/abs/2305.14283v3)  **Published Date:** 2023-05-23\\n- **LangChain:**\\n\\n   - **Template:** [rewrite-retrieve-read](https://python.langchain.com/docs/templates/rewrite-retrieve-read)\\n   - **Cookbook:** [rewrite](https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb)\\n\\n**Abstract:** Large Language Models (LLMs) play powerful, black-box readers in the\\nretrieve-then-read pipeline, making remarkable progress in knowledge-intensive\\ntasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of\\nthe previous retrieve-then-read for the retrieval-augmented LLMs from the\\nperspective of the query rewriting. Unlike prior studies focusing on adapting\\neither the retriever or the reader, our approach pays attention to the\\nadaptation of the search query itself, for there is inevitably a gap between\\nthe input text and the needed knowledge in retrieval. We first prompt an LLM to\\ngenerate the query, then use a web search engine to retrieve contexts.\\nFurthermore, to better align the query to the frozen modules, we propose a\\ntrainable scheme for our pipeline. A small language model is adopted as a\\ntrainable rewriter to cater to the black-box LLM reader. The rewriter is\\ntrained using the feedback of the LLM reader by reinforcement learning.\\nEvaluation is conducted on downstream tasks, open-domain QA and multiple-choice\\nQA. Experiments results show consistent performance improvement, indicating\\nthat our framework is proven effective and scalable, and brings a new framework\\nfor retrieval-augmented LLM.\\n\\n## Large Language Model Guided Tree-of-Thought\\n\\n- **Authors:** Jieyi Long\\n- **arXiv id:** [2305.08291v1](http://arxiv.org/abs/2305.08291v1)  **Published Date:** 2023-05-15\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.tot](https://python.langchain.com/api_reference/experimental/tot.html)\\n   - **Cookbook:** [tree_of_thought](https://github.com/langchain-ai/langchain/blob/master/cookbook/tree_of_thought.ipynb)\\n\\n**Abstract:** In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel\\napproach aimed at improving the problem-solving capabilities of auto-regressive\\nlarge language models (LLMs). The ToT technique is inspired by the human mind\\'s\\napproach for solving complex reasoning tasks through trial and error. In this\\nprocess, the human mind explores the solution space through a tree-like thought\\nprocess, allowing for backtracking when necessary. To implement ToT as a\\nsoftware system, we augment an LLM with additional modules including a prompter\\nagent, a checker module, a memory module, and a ToT controller. In order to\\nsolve a given problem, these modules engage in a multi-round conversation with\\nthe LLM. The memory module records the conversation and state history of the\\nproblem solving process, which allows the system to backtrack to the previous\\nsteps of the thought-process and explore other directions from there. To verify\\nthe effectiveness of the proposed technique, we implemented a ToT-based solver\\nfor the Sudoku Puzzle. Experimental results show that the ToT framework can\\nsignificantly increase the success rate of Sudoku puzzle solving. Our\\nimplementation of the ToT-based Sudoku solver is available on [GitHub](https://github.com/jieyilong/tree-of-thought-puzzle-solver).\\n\\n## Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models\\n\\n- **Authors:** Lei Wang, Wanyu Xu, Yihuai Lan,  et al.\\n- **arXiv id:** [2305.04091v3](http://arxiv.org/abs/2305.04091v3)  **Published Date:** 2023-05-06\\n- **LangChain:**\\n\\n   - **Cookbook:** [plan_and_execute_agent](https://github.com/langchain-ai/langchain/blob/master/cookbook/plan_and_execute_agent.ipynb)\\n\\n**Abstract:** Large language models (LLMs) have recently been shown to deliver impressive\\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\\nreasoning steps and improve their reasoning task accuracy. To eliminate the\\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\\n\"Let\\'s think step by step\" as an input prompt to LLMs. Despite the success of\\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\\nmissing-step errors, and semantic misunderstanding errors. To address the\\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\\ntwo components: first, devising a plan to divide the entire task into smaller\\nsubtasks, and then carrying out the subtasks according to the plan. To address\\nthe calculation errors and improve the quality of generated reasoning steps, we\\nextend PS prompting with more detailed instructions and derive PS+ prompting.\\nWe evaluate our proposed prompting strategy on ten datasets across three\\nreasoning problems. The experimental results over GPT-3 show that our proposed\\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\\nreasoning problem. The code can be found at\\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.\\n\\n## Zero-Shot Listwise Document Reranking with a Large Language Model\\n\\n- **Authors:** Xueguang Ma, Xinyu Zhang, Ronak Pradeep,  et al.\\n- **arXiv id:** [2305.02156v1](http://arxiv.org/abs/2305.02156v1)  **Published Date:** 2023-05-03\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/how_to/contextual_compression](https://python.langchain.com/docs/how_to/contextual_compression)\\n   - **API Reference:** [langchain...LLMListwiseRerank](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank.html#)\\n\\n**Abstract:** Supervised ranking methods based on bi-encoder or cross-encoder architectures\\nhave shown success in multi-stage text ranking tasks, but they require large\\namounts of relevance judgments as training data. In this work, we propose\\nListwise Reranker with a Large Language Model (LRL), which achieves strong\\nreranking effectiveness without using any task-specific training data.\\nDifferent from the existing pointwise ranking methods, where documents are\\nscored independently and ranked according to the scores, LRL directly generates\\na reordered list of document identifiers given the candidate documents.\\nExperiments on three TREC web search datasets demonstrate that LRL not only\\noutperforms zero-shot pointwise methods when reranking first-stage retrieval\\nresults, but can also act as a final-stage reranker to improve the top-ranked\\nresults of a pointwise method for improved efficiency. Additionally, we apply\\nour approach to subsets of MIRACL, a recent multilingual retrieval dataset,\\nwith results showing its potential to generalize across different languages.\\n\\n## Visual Instruction Tuning\\n\\n- **Authors:** Haotian Liu, Chunyuan Li, Qingyang Wu,  et al.\\n- **arXiv id:** [2304.08485v2](http://arxiv.org/abs/2304.08485v2)  **Published Date:** 2023-04-17\\n- **LangChain:**\\n\\n   - **Cookbook:** [Semi_structured_multi_modal_RAG_LLaMA2](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb), [Semi_structured_and_multi_modal_RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb)\\n\\n**Abstract:** Instruction tuning large language models (LLMs) using machine-generated\\ninstruction-following data has improved zero-shot capabilities on new tasks,\\nbut the idea is less explored in the multimodal field. In this paper, we\\npresent the first attempt to use language-only GPT-4 to generate multimodal\\nlanguage-image instruction-following data. By instruction tuning on such\\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\\nend-to-end trained large multimodal model that connects a vision encoder and\\nLLM for general-purpose visual and language understanding.Our early experiments\\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\\nGPT-4 generated visual instruction tuning data, our model and code base\\npublicly available.\\n\\n## Generative Agents: Interactive Simulacra of Human Behavior\\n\\n- **Authors:** Joon Sung Park, Joseph C. O\\'Brien, Carrie J. Cai,  et al.\\n- **arXiv id:** [2304.03442v2](http://arxiv.org/abs/2304.03442v2)  **Published Date:** 2023-04-07\\n- **LangChain:**\\n\\n   - **Cookbook:** [generative_agents_interactive_simulacra_of_human_behavior](https://github.com/langchain-ai/langchain/blob/master/cookbook/generative_agents_interactive_simulacra_of_human_behavior.ipynb), [multiagent_bidding](https://github.com/langchain-ai/langchain/blob/master/cookbook/multiagent_bidding.ipynb)\\n\\n**Abstract:** Believable proxies of human behavior can empower interactive applications\\nranging from immersive environments to rehearsal spaces for interpersonal\\ncommunication to prototyping tools. In this paper, we introduce generative\\nagents--computational software agents that simulate believable human behavior.\\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\\nwhile authors write; they form opinions, notice each other, and initiate\\nconversations; they remember and reflect on days past as they plan the next\\nday. To enable generative agents, we describe an architecture that extends a\\nlarge language model to store a complete record of the agent\\'s experiences\\nusing natural language, synthesize those memories over time into higher-level\\nreflections, and retrieve them dynamically to plan behavior. We instantiate\\ngenerative agents to populate an interactive sandbox environment inspired by\\nThe Sims, where end users can interact with a small town of twenty five agents\\nusing natural language. In an evaluation, these generative agents produce\\nbelievable individual and emergent social behaviors: for example, starting with\\nonly a single user-specified notion that one agent wants to throw a Valentine\\'s\\nDay party, the agents autonomously spread invitations to the party over the\\nnext two days, make new acquaintances, ask each other out on dates to the\\nparty, and coordinate to show up for the party together at the right time. We\\ndemonstrate through ablation that the components of our agent\\narchitecture--observation, planning, and reflection--each contribute critically\\nto the believability of agent behavior. By fusing large language models with\\ncomputational, interactive agents, this work introduces architectural and\\ninteraction patterns for enabling believable simulations of human behavior.\\n\\n## CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society\\n\\n- **Authors:** Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani,  et al.\\n- **arXiv id:** [2303.17760v2](http://arxiv.org/abs/2303.17760v2)  **Published Date:** 2023-03-31\\n- **LangChain:**\\n\\n   - **Cookbook:** [camel_role_playing](https://github.com/langchain-ai/langchain/blob/master/cookbook/camel_role_playing.ipynb)\\n\\n**Abstract:** The rapid advancement of chat-based language models has led to remarkable\\nprogress in complex task-solving. However, their success heavily relies on\\nhuman input to guide the conversation, which can be challenging and\\ntime-consuming. This paper explores the potential of building scalable\\ntechniques to facilitate autonomous cooperation among communicative agents, and\\nprovides insight into their \"cognitive\" processes. To address the challenges of\\nachieving autonomous cooperation, we propose a novel communicative agent\\nframework named role-playing. Our approach involves using inception prompting\\nto guide chat agents toward task completion while maintaining consistency with\\nhuman intentions. We showcase how role-playing can be used to generate\\nconversational data for studying the behaviors and capabilities of a society of\\nagents, providing a valuable resource for investigating conversational language\\nmodels. In particular, we conduct comprehensive studies on\\ninstruction-following cooperation in multi-agent settings. Our contributions\\ninclude introducing a novel communicative agent framework, offering a scalable\\napproach for studying the cooperative behaviors and capabilities of multi-agent\\nsystems, and open-sourcing our library to support research on communicative\\nagents and beyond: https://github.com/camel-ai/camel.\\n\\n## HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\\n\\n- **Authors:** Yongliang Shen, Kaitao Song, Xu Tan,  et al.\\n- **arXiv id:** [2303.17580v4](http://arxiv.org/abs/2303.17580v4)  **Published Date:** 2023-03-30\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.autonomous_agents](https://python.langchain.com/api_reference/experimental/autonomous_agents.html)\\n   - **Cookbook:** [hugginggpt](https://github.com/langchain-ai/langchain/blob/master/cookbook/hugginggpt.ipynb)\\n\\n**Abstract:** Solving complicated AI tasks with different domains and modalities is a key\\nstep toward artificial general intelligence. While there are numerous AI models\\navailable for various domains and modalities, they cannot handle complicated AI\\ntasks autonomously. Considering large language models (LLMs) have exhibited\\nexceptional abilities in language understanding, generation, interaction, and\\nreasoning, we advocate that LLMs could act as a controller to manage existing\\nAI models to solve complicated AI tasks, with language serving as a generic\\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\\nrequest, select models according to their function descriptions available in\\nHugging Face, execute each subtask with the selected AI model, and summarize\\nthe response according to the execution results. By leveraging the strong\\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\\nmodalities and domains and achieve impressive results in language, vision,\\nspeech, and other challenging tasks, which paves a new way towards the\\nrealization of artificial general intelligence.\\n\\n## A Watermark for Large Language Models\\n\\n- **Authors:** John Kirchenbauer, Jonas Geiping, Yuxin Wen,  et al.\\n- **arXiv id:** [2301.10226v4](http://arxiv.org/abs/2301.10226v4)  **Published Date:** 2023-01-24\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_community...OCIModelDeploymentTGI](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI.html#langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI), [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)\\n\\n**Abstract:** Potential harms of large language models can be mitigated by watermarking\\nmodel output, i.e., embedding signals into generated text that are invisible to\\nhumans but algorithmically detectable from a short span of tokens. We propose a\\nwatermarking framework for proprietary language models. The watermark can be\\nembedded with negligible impact on text quality, and can be detected using an\\nefficient open-source algorithm without access to the language model API or\\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\\nbefore a word is generated, and then softly promoting use of green tokens\\nduring sampling. We propose a statistical test for detecting the watermark with\\ninterpretable p-values, and derive an information-theoretic framework for\\nanalyzing the sensitivity of the watermark. We test the watermark using a\\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\\nfamily, and discuss robustness and security.\\n\\n## Precise Zero-Shot Dense Retrieval without Relevance Labels\\n\\n- **Authors:** Luyu Gao, Xueguang Ma, Jimmy Lin,  et al.\\n- **arXiv id:** [2212.10496v1](http://arxiv.org/abs/2212.10496v1)  **Published Date:** 2022-12-20\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **API Reference:** [langchain...HypotheticalDocumentEmbedder](https://api.python.langchain.com/en/latest/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html#langchain.chains.hyde.base.HypotheticalDocumentEmbedder)\\n   - **Template:** [hyde](https://python.langchain.com/docs/templates/hyde)\\n   - **Cookbook:** [hypothetical_document_embeddings](https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb)\\n\\n**Abstract:** While dense retrieval has been shown effective and efficient across tasks and\\nlanguages, it remains difficult to create effective fully zero-shot dense\\nretrieval systems when no relevance label is available. In this paper, we\\nrecognize the difficulty of zero-shot learning and encoding relevance. Instead,\\nwe propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a\\nquery, HyDE first zero-shot instructs an instruction-following language model\\n(e.g. InstructGPT) to generate a hypothetical document. The document captures\\nrelevance patterns but is unreal and may contain false details. Then, an\\nunsupervised contrastively learned encoder~(e.g. Contriever) encodes the\\ndocument into an embedding vector. This vector identifies a neighborhood in the\\ncorpus embedding space, where similar real documents are retrieved based on\\nvector similarity. This second step ground the generated document to the actual\\ncorpus, with the encoder\\'s dense bottleneck filtering out the incorrect\\ndetails. Our experiments show that HyDE significantly outperforms the\\nstate-of-the-art unsupervised dense retriever Contriever and shows strong\\nperformance comparable to fine-tuned retrievers, across various tasks (e.g. web\\nsearch, QA, fact verification) and languages~(e.g. sw, ko, ja).\\n\\n## Constitutional AI: Harmlessness from AI Feedback\\n\\n- **Authors:** Yuntao Bai, Saurav Kadavath, Sandipan Kundu,  et al.\\n- **arXiv id:** [2212.08073v1](http://arxiv.org/abs/2212.08073v1)  **Published Date:** 2022-12-15\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/versions/migrating_chains/constitutional_chain](https://python.langchain.com/docs/versions/migrating_chains/constitutional_chain)\\n\\n**Abstract:** As AI systems become more capable, we would like to enlist their help to\\nsupervise other AIs. We experiment with methods for training a harmless AI\\nassistant through self-improvement, without any human labels identifying\\nharmful outputs. The only human oversight is provided through a list of rules\\nor principles, and so we refer to the method as \\'Constitutional AI\\'. The\\nprocess involves both a supervised learning and a reinforcement learning phase.\\nIn the supervised phase we sample from an initial model, then generate\\nself-critiques and revisions, and then finetune the original model on revised\\nresponses. In the RL phase, we sample from the finetuned model, use a model to\\nevaluate which of the two samples is better, and then train a preference model\\nfrom this dataset of AI preferences. We then train with RL using the preference\\nmodel as the reward signal, i.e. we use \\'RL from AI Feedback\\' (RLAIF). As a\\nresult we are able to train a harmless but non-evasive AI assistant that\\nengages with harmful queries by explaining its objections to them. Both the SL\\nand RL methods can leverage chain-of-thought style reasoning to improve the\\nhuman-judged performance and transparency of AI decision making. These methods\\nmake it possible to control AI behavior more precisely and with far fewer human\\nlabels.\\n\\n## Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments\\n\\n- **Authors:** Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande,  et al.\\n- **arXiv id:** [2212.07425v3](http://arxiv.org/abs/2212.07425v3)  **Published Date:** 2022-12-12\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.fallacy_removal](https://python.langchain.com/api_reference/experimental/fallacy_removal.html)\\n\\n**Abstract:** The spread of misinformation, propaganda, and flawed argumentation has been\\namplified in the Internet era. Given the volume of data and the subtlety of\\nidentifying violations of argumentation norms, supporting information analytics\\ntasks, like content moderation, with trustworthy methods that can identify\\nlogical fallacies is essential. In this paper, we formalize prior theoretical\\nwork on logical fallacies into a comprehensive three-stage evaluation framework\\nof detection, coarse-grained, and fine-grained classification. We adapt\\nexisting evaluation datasets for each stage of the evaluation. We employ three\\nfamilies of robust and explainable methods based on prototype reasoning,\\ninstance-based reasoning, and knowledge injection. The methods combine language\\nmodels with background knowledge and explainable mechanisms. Moreover, we\\naddress data sparsity with strategies for data augmentation and curriculum\\nlearning. Our three-stage framework natively consolidates prior datasets and\\nmethods from existing tasks, like propaganda detection, serving as an\\noverarching evaluation testbed. We extensively evaluate these methods on our\\ndatasets, focusing on their robustness and explainability. Our results provide\\ninsight into the strengths and weaknesses of the methods on different\\ncomponents and fallacy classes, indicating that fallacy identification is a\\nchallenging task that may require specialized forms of reasoning to capture\\nvarious classes. We share our open-source code and data on GitHub to support\\nfurther work on logical fallacy identification.\\n\\n## Complementary Explanations for Effective In-Context Learning\\n\\n- **Authors:** Xi Ye, Srinivasan Iyer, Asli Celikyilmaz,  et al.\\n- **arXiv id:** [2211.13892v2](http://arxiv.org/abs/2211.13892v2)  **Published Date:** 2022-11-25\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_core...MaxMarginalRelevanceExampleSelector](https://api.python.langchain.com/en/latest/example_selectors/langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector.html#langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector)\\n\\n**Abstract:** Large language models (LLMs) have exhibited remarkable capabilities in\\nlearning from explanations in prompts, but there has been limited understanding\\nof exactly how these explanations function or why they are effective. This work\\naims to better understand the mechanisms by which explanations are used for\\nin-context learning. We first study the impact of two different factors on the\\nperformance of prompts with explanations: the computation trace (the way the\\nsolution is decomposed) and the natural language used to express the prompt. By\\nperturbing explanations on three controlled tasks, we show that both factors\\ncontribute to the effectiveness of explanations. We further study how to form\\nmaximally effective sets of explanations for solving a given test query. We\\nfind that LLMs can benefit from the complementarity of the explanation set:\\ndiverse reasoning skills shown by different exemplars can lead to better\\nperformance. Therefore, we propose a maximal marginal relevance-based exemplar\\nselection approach for constructing exemplar sets that are both relevant as\\nwell as complementary, which successfully improves the in-context learning\\nperformance across three real-world tasks on multiple LLMs.\\n\\n## PAL: Program-aided Language Models\\n\\n- **Authors:** Luyu Gao, Aman Madaan, Shuyan Zhou,  et al.\\n- **arXiv id:** [2211.10435v2](http://arxiv.org/abs/2211.10435v2)  **Published Date:** 2022-11-18\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.pal_chain](https://python.langchain.com/api_reference/experimental/pal_chain.html), [langchain_experimental...PALChain](https://api.python.langchain.com/en/latest/pal_chain/langchain_experimental.pal_chain.base.PALChain.html#langchain_experimental.pal_chain.base.PALChain)\\n   - **Cookbook:** [program_aided_language_model](https://github.com/langchain-ai/langchain/blob/master/cookbook/program_aided_language_model.ipynb)\\n\\n**Abstract:** Large language models (LLMs) have recently demonstrated an impressive ability\\nto perform arithmetic and symbolic reasoning tasks, when provided with a few\\nexamples at test time (\"few-shot prompting\"). Much of this success can be\\nattributed to prompting methods such as \"chain-of-thought\\'\\', which employ LLMs\\nfor both understanding the problem description by decomposing it into steps, as\\nwell as solving each step of the problem. While LLMs seem to be adept at this\\nsort of step-by-step decomposition, LLMs often make logical and arithmetic\\nmistakes in the solution part, even when the problem is decomposed correctly.\\nIn this paper, we present Program-Aided Language models (PAL): a novel approach\\nthat uses the LLM to read natural language problems and generate programs as\\nthe intermediate reasoning steps, but offloads the solution step to a runtime\\nsuch as a Python interpreter. With PAL, decomposing the natural language\\nproblem into runnable steps remains the only learning task for the LLM, while\\nsolving is delegated to the interpreter. We demonstrate this synergy between a\\nneural LLM and a symbolic interpreter across 13 mathematical, symbolic, and\\nalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all\\nthese natural language reasoning tasks, generating code using an LLM and\\nreasoning using a Python interpreter leads to more accurate results than much\\nlarger models. For example, PAL using Codex achieves state-of-the-art few-shot\\naccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B\\nwhich uses chain-of-thought by absolute 15% top-1. Our code and data are\\npublicly available at http://reasonwithpal.com/ .\\n\\n## An Analysis of Fusion Functions for Hybrid Retrieval\\n\\n- **Authors:** Sebastian Bruch, Siyu Gai, Amir Ingber\\n- **arXiv id:** [2210.11934v2](http://arxiv.org/abs/2210.11934v2)  **Published Date:** 2022-10-21\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n\\n**Abstract:** We study hybrid search in text retrieval where lexical and semantic search\\nare fused together with the intuition that the two are complementary in how\\nthey model relevance. In particular, we examine fusion by a convex combination\\n(CC) of lexical and semantic scores, as well as the Reciprocal Rank Fusion\\n(RRF) method, and identify their advantages and potential pitfalls. Contrary to\\nexisting studies, we find RRF to be sensitive to its parameters; that the\\nlearning of a CC fusion is generally agnostic to the choice of score\\nnormalization; that CC outperforms RRF in in-domain and out-of-domain settings;\\nand finally, that CC is sample efficient, requiring only a small set of\\ntraining examples to tune its only parameter to a target domain.\\n\\n## ReAct: Synergizing Reasoning and Acting in Language Models\\n\\n- **Authors:** Shunyu Yao, Jeffrey Zhao, Dian Yu,  et al.\\n- **arXiv id:** [2210.03629v3](http://arxiv.org/abs/2210.03629v3)  **Published Date:** 2022-10-06\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/integrations/tools/ionic_shopping](https://python.langchain.com/docs/integrations/tools/ionic_shopping), [docs/integrations/providers/cohere](https://python.langchain.com/docs/integrations/providers/cohere), [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **API Reference:** [langchain...create_react_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.react.agent.create_react_agent.html#langchain.agents.react.agent.create_react_agent), [langchain...TrajectoryEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain)\\n\\n**Abstract:** While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io\\n\\n## Deep Lake: a Lakehouse for Deep Learning\\n\\n- **Authors:** Sasun Hambardzumyan, Abhinav Tuli, Levon Ghukasyan,  et al.\\n- **arXiv id:** [2209.10785v2](http://arxiv.org/abs/2209.10785v2)  **Published Date:** 2022-09-22\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/integrations/providers/activeloop_deeplake](https://python.langchain.com/docs/integrations/providers/activeloop_deeplake)\\n\\n**Abstract:** Traditional data lakes provide critical data infrastructure for analytical\\nworkloads by enabling time travel, running SQL queries, ingesting data with\\nACID transactions, and visualizing petabyte-scale datasets on cloud storage.\\nThey allow organizations to break down data silos, unlock data-driven\\ndecision-making, improve operational efficiency, and reduce costs. However, as\\ndeep learning usage increases, traditional data lakes are not well-designed for\\napplications such as natural language processing (NLP), audio processing,\\ncomputer vision, and applications involving non-tabular datasets. This paper\\npresents Deep Lake, an open-source lakehouse for deep learning applications\\ndeveloped at Activeloop. Deep Lake maintains the benefits of a vanilla data\\nlake with one key difference: it stores complex data, such as images, videos,\\nannotations, as well as tabular data, in the form of tensors and rapidly\\nstreams the data over the network to (a) Tensor Query Language, (b) in-browser\\nvisualization engine, or (c) deep learning frameworks without sacrificing GPU\\nutilization. Datasets stored in Deep Lake can be accessed from PyTorch,\\nTensorFlow, JAX, and integrate with numerous MLOps tools.\\n\\n## Matryoshka Representation Learning\\n\\n- **Authors:** Aditya Kusupati, Gantavya Bhatt, Aniket Rege,  et al.\\n- **arXiv id:** [2205.13147v4](http://arxiv.org/abs/2205.13147v4)  **Published Date:** 2022-05-26\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/integrations/providers/snowflake](https://python.langchain.com/docs/integrations/providers/snowflake)\\n\\n**Abstract:** Learned representations are a central component in modern ML systems, serving\\na multitude of downstream tasks. When training such representations, it is\\noften the case that computational and statistical constraints for each\\ndownstream task are unknown. In this context rigid, fixed capacity\\nrepresentations can be either over or under-accommodating to the task at hand.\\nThis leads us to ask: can we design a flexible representation that can adapt to\\nmultiple downstream tasks with varying computational resources? Our main\\ncontribution is Matryoshka Representation Learning (MRL) which encodes\\ninformation at different granularities and allows a single embedding to adapt\\nto the computational constraints of downstream tasks. MRL minimally modifies\\nexisting representation learning pipelines and imposes no additional cost\\nduring inference and deployment. MRL learns coarse-to-fine representations that\\nare at least as accurate and rich as independently trained low-dimensional\\nrepresentations. The flexibility within the learned Matryoshka Representations\\noffer: (a) up to 14x smaller embedding size for ImageNet-1K classification at\\nthe same level of accuracy; (b) up to 14x real-world speed-ups for large-scale\\nretrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for\\nlong-tail few-shot classification, all while being as robust as the original\\nrepresentations. Finally, we show that MRL extends seamlessly to web-scale\\ndatasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),\\nvision + language (ALIGN) and language (BERT). MRL code and pretrained models\\nare open-sourced at https://github.com/RAIVNLab/MRL.\\n\\n## Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages\\n\\n- **Authors:** Kevin Heffernan, Onur Çelebi, Holger Schwenk\\n- **arXiv id:** [2205.12654v1](http://arxiv.org/abs/2205.12654v1)  **Published Date:** 2022-05-25\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_community...LaserEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.laser.LaserEmbeddings.html#langchain_community.embeddings.laser.LaserEmbeddings)\\n\\n**Abstract:** Scaling multilingual representation learning beyond the hundred most frequent\\nlanguages is challenging, in particular to cover the long tail of low-resource\\nlanguages. A promising approach has been to train one-for-all multilingual\\nmodels capable of cross-lingual transfer, but these models often suffer from\\ninsufficient capacity and interference between unrelated languages. Instead, we\\nmove away from this approach and focus on training multiple language (family)\\nspecific representations, but most prominently enable all languages to still be\\nencoded in the same representational space. To achieve this, we focus on\\nteacher-student training, allowing all encoders to be mutually compatible for\\nbitext mining, and enabling fast learning of new languages. We introduce a new\\nteacher-student training scheme which combines supervised and self-supervised\\ntraining, allowing encoders to take advantage of monolingual training data,\\nwhich is valuable in the low-resource setting.\\n  Our approach significantly outperforms the original LASER encoder. We study\\nvery low-resource languages and handle 50 African languages, many of which are\\nnot covered by any other model. For these languages, we train sentence\\nencoders, mine bitexts, and validate the bitexts by training NMT systems.\\n\\n## Evaluating the Text-to-SQL Capabilities of Large Language Models\\n\\n- **Authors:** Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau\\n- **arXiv id:** [2204.00498v1](http://arxiv.org/abs/2204.00498v1)  **Published Date:** 2022-03-15\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/tutorials/sql_qa](https://python.langchain.com/docs/tutorials/sql_qa)\\n   - **API Reference:** [langchain_community...SQLDatabase](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.sql_database.SQLDatabase.html#langchain_community.utilities.sql_database.SQLDatabase), [langchain_community...SparkSQL](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.spark_sql.SparkSQL.html#langchain_community.utilities.spark_sql.SparkSQL)\\n\\n**Abstract:** We perform an empirical evaluation of Text-to-SQL capabilities of the Codex\\nlanguage model. We find that, without any finetuning, Codex is a strong\\nbaseline on the Spider benchmark; we also analyze the failure modes of Codex in\\nthis setting. Furthermore, we demonstrate on the GeoQuery and Scholar\\nbenchmarks that a small number of in-domain examples provided in the prompt\\nenables Codex to perform better than state-of-the-art models finetuned on such\\nfew-shot examples.\\n\\n## Locally Typical Sampling\\n\\n- **Authors:** Clara Meister, Tiago Pimentel, Gian Wiher,  et al.\\n- **arXiv id:** [2202.00666v5](http://arxiv.org/abs/2202.00666v5)  **Published Date:** 2022-02-01\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)\\n\\n**Abstract:** Today\\'s probabilistic language generators fall short when it comes to\\nproducing coherent and fluent text despite the fact that the underlying models\\nperform well under standard metrics, e.g., perplexity. This discrepancy has\\npuzzled the language generation community for the last few years. In this work,\\nwe posit that the abstraction of natural language generation as a discrete\\nstochastic process--which allows for an information-theoretic analysis--can\\nprovide new insights into the behavior of probabilistic language generators,\\ne.g., why high-probability texts can be dull or repetitive. Humans use language\\nas a means of communicating information, aiming to do so in a simultaneously\\nefficient and error-minimizing manner; in fact, psycholinguistics research\\nsuggests humans choose each word in a string with this subconscious goal in\\nmind. We formally define the set of strings that meet this criterion: those for\\nwhich each word has an information content close to the expected information\\ncontent, i.e., the conditional entropy of our model. We then propose a simple\\nand efficient procedure for enforcing this criterion when generating from\\nprobabilistic models, which we call locally typical sampling. Automatic and\\nhuman evaluations show that, in comparison to nucleus and top-k sampling,\\nlocally typical sampling offers competitive performance (in both abstractive\\nsummarization and story generation) in terms of quality while consistently\\nreducing degenerate repetitions.\\n\\n## ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\\n\\n- **Authors:** Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,  et al.\\n- **arXiv id:** [2112.01488v3](http://arxiv.org/abs/2112.01488v3)  **Published Date:** 2021-12-02\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/integrations/retrievers/ragatouille](https://python.langchain.com/docs/integrations/retrievers/ragatouille), [docs/integrations/providers/ragatouille](https://python.langchain.com/docs/integrations/providers/ragatouille), [docs/concepts](https://python.langchain.com/docs/concepts)\\n\\n**Abstract:** Neural information retrieval (IR) has greatly advanced search and other\\nknowledge-intensive language tasks. While many neural IR methods encode queries\\nand documents into single-vector representations, late interaction models\\nproduce multi-vector representations at the granularity of each token and\\ndecompose relevance modeling into scalable token-level computations. This\\ndecomposition has been shown to make late interaction more effective, but it\\ninflates the space footprint of these models by an order of magnitude. In this\\nwork, we introduce ColBERTv2, a retriever that couples an aggressive residual\\ncompression mechanism with a denoised supervision strategy to simultaneously\\nimprove the quality and space footprint of late interaction. We evaluate\\nColBERTv2 across a wide range of benchmarks, establishing state-of-the-art\\nquality within and outside the training domain while reducing the space\\nfootprint of late interaction models by 6--10$\\\\times$.\\n\\n## Learning Transferable Visual Models From Natural Language Supervision\\n\\n- **Authors:** Alec Radford, Jong Wook Kim, Chris Hallacy,  et al.\\n- **arXiv id:** [2103.00020v1](http://arxiv.org/abs/2103.00020v1)  **Published Date:** 2021-02-26\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.open_clip](https://python.langchain.com/api_reference/experimental/open_clip.html)\\n\\n**Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined object categories. This restricted form of supervision limits\\ntheir generality and usability since additional labeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a promising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple pre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to learn\\nSOTA image representations from scratch on a dataset of 400 million (image,\\ntext) pairs collected from the internet. After pre-training, natural language\\nis used to reference learned visual concepts (or describe new ones) enabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by benchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action recognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel transfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need for any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.\\n\\n## Language Models are Few-Shot Learners\\n\\n- **Authors:** Tom B. Brown, Benjamin Mann, Nick Ryder,  et al.\\n- **arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4)  **Published Date:** 2020-05-28\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n\\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\\na specific task. While typically task-agnostic in architecture, this method\\nstill requires task-specific fine-tuning datasets of thousands or tens of\\nthousands of examples. By contrast, humans can generally perform a new language\\ntask from only a few examples or from simple instructions - something which\\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\\neven reaching competitiveness with prior state-of-the-art fine-tuning\\napproaches. Specifically, we train GPT-3, an autoregressive language model with\\n175 billion parameters, 10x more than any previous non-sparse language model,\\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\\napplied without any gradient updates or fine-tuning, with tasks and few-shot\\ndemonstrations specified purely via text interaction with the model. GPT-3\\nachieves strong performance on many NLP datasets, including translation,\\nquestion-answering, and cloze tasks, as well as several tasks that require\\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\\nwe also identify some datasets where GPT-3\\'s few-shot learning still struggles,\\nas well as some datasets where GPT-3 faces methodological issues related to\\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\\nof news articles which human evaluators have difficulty distinguishing from\\narticles written by humans. We discuss broader societal impacts of this finding\\nand of GPT-3 in general.\\n\\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n- **Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\\n- **arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4)  **Published Date:** 2020-05-22\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n\\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.\\n\\n## CTRL: A Conditional Transformer Language Model for Controllable Generation\\n\\n- **Authors:** Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al.\\n- **arXiv id:** [1909.05858v2](http://arxiv.org/abs/1909.05858v2)  **Published Date:** 2019-09-11\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)\\n\\n**Abstract:** Large-scale language models show promising text generation capabilities, but\\nusers cannot easily control particular aspects of the generated text. We\\nrelease CTRL, a 1.63 billion-parameter conditional transformer language model,\\ntrained to condition on control codes that govern style, content, and\\ntask-specific behavior. Control codes were derived from structure that\\nnaturally co-occurs with raw text, preserving the advantages of unsupervised\\nlearning while providing more explicit control over text generation. These\\ncodes also allow CTRL to predict which parts of the training data are most\\nlikely given a sequence. This provides a potential method for analyzing large\\namounts of data via model-based source attribution. We have released multiple\\nfull-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.\\n'), Document(id='03a5900f-44b8-41b9-a517-b2308e3b6580', metadata={'file_path': 'cookbook\\\\sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx', 'source': 'cookbook\\\\sql_db_qa.mdx'}, page_content='# SQL Database Chain\\n\\nThis example demonstrates the use of the `SQLDatabaseChain` for answering questions over a SQL database.\\n\\nUnder the hood, LangChain uses SQLAlchemy to connect to SQL databases. The `SQLDatabaseChain` can therefore be used with any SQL dialect supported by SQLAlchemy, such as MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, [Databricks](/docs/ecosystem/integrations/databricks.html) and SQLite. Please refer to the SQLAlchemy documentation for more information about requirements for connecting to your database. For example, a connection to MySQL requires an appropriate connector such as PyMySQL. A URI for a MySQL connection might look like: `mysql+pymysql://user:pass@some_mysql_db_address/db_name`.\\n\\nThis demonstration uses SQLite and the example Chinook database.\\nTo set it up, follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the `.db` file in a notebooks folder at the root of this repository.\\n\\n\\n```python\\nfrom langchain_openai import OpenAI\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_experimental.sql import SQLDatabaseChain\\n```\\n\\n\\n```python\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\nllm = OpenAI(temperature=0, verbose=True)\\n```\\n\\n**NOTE:** For data-sensitive projects, you can specify `return_direct=True` in the `SQLDatabaseChain` initialization to directly return the output of the SQL query without any additional formatting. This prevents the LLM from seeing any contents within the database. Note, however, the LLM still has access to the database scheme (i.e. dialect, table and key names) by default.\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\\n```\\n\\n\\n```python\\ndb_chain.run(\"How many employees are there?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many employees are there?\\n    SQLQuery:\\n\\n    /workspace/langchain/langchain/sql_database.py:191: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\\n      sample_rows = connection.execute(command)\\n\\n\\n    SELECT COUNT(*) FROM \"Employee\";\\n    SQLResult: [(8,)]\\n    Answer:There are 8 employees.\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    \\'There are 8 employees.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n## Use Query Checker\\nSometimes the Language Model generates invalid SQL with small mistakes that can be self-corrected using the same technique used by the SQL Database Agent to try and fix the SQL using the LLM. You can simply specify this option when creating the chain:\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True)\\n```\\n\\n\\n```python\\ndb_chain.run(\"How many albums by Aerosmith?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many albums by Aerosmith?\\n    SQLQuery:SELECT COUNT(*) FROM Album WHERE ArtistId = 3;\\n    SQLResult: [(1,)]\\n    Answer:There is 1 album by Aerosmith.\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    \\'There is 1 album by Aerosmith.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n## Customize Prompt\\nYou can also customize the prompt that is used. Here is an example prompting it to understand that foobar is the same as the Employee table\\n\\n\\n```python\\nfrom langchain.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\\nUse the following format:\\n\\nQuestion: \"Question here\"\\nSQLQuery: \"SQL Query to run\"\\nSQLResult: \"Result of the SQLQuery\"\\nAnswer: \"Final answer here\"\\n\\nOnly use the following tables:\\n\\n{table_info}\\n\\nIf someone asks for the table foobar, they really mean the employee table.\\n\\nQuestion: {input}\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"dialect\"], template=_DEFAULT_TEMPLATE\\n)\\n```\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True)\\n```\\n\\n\\n```python\\ndb_chain.run(\"How many employees are there in the foobar table?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many employees are there in the foobar table?\\n    SQLQuery:SELECT COUNT(*) FROM Employee;\\n    SQLResult: [(8,)]\\n    Answer:There are 8 employees in the foobar table.\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    \\'There are 8 employees in the foobar table.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n## Return Intermediate Steps\\n\\nYou can also return the intermediate steps of the SQLDatabaseChain. This allows you to access the SQL statement that was generated, as well as the result of running that against the SQL Database.\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True, use_query_checker=True, return_intermediate_steps=True)\\n```\\n\\n\\n```python\\nresult = db_chain(\"How many employees are there in the foobar table?\")\\nresult[\"intermediate_steps\"]\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many employees are there in the foobar table?\\n    SQLQuery:SELECT COUNT(*) FROM Employee;\\n    SQLResult: [(8,)]\\n    Answer:There are 8 employees in the foobar table.\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    [{\\'input\\': \\'How many employees are there in the foobar table?\\\\nSQLQuery:SELECT COUNT(*) FROM Employee;\\\\nSQLResult: [(8,)]\\\\nAnswer:\\',\\n      \\'top_k\\': \\'5\\',\\n      \\'dialect\\': \\'sqlite\\',\\n      \\'table_info\\': \\'\\\\nCREATE TABLE \"Artist\" (\\\\n\\\\t\"ArtistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"ArtistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Artist table:\\\\nArtistId\\\\tName\\\\n1\\\\tAC/DC\\\\n2\\\\tAccept\\\\n3\\\\tAerosmith\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Employee\" (\\\\n\\\\t\"EmployeeId\" INTEGER NOT NULL, \\\\n\\\\t\"LastName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"FirstName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"Title\" NVARCHAR(30), \\\\n\\\\t\"ReportsTo\" INTEGER, \\\\n\\\\t\"BirthDate\" DATETIME, \\\\n\\\\t\"HireDate\" DATETIME, \\\\n\\\\t\"Address\" NVARCHAR(70), \\\\n\\\\t\"City\" NVARCHAR(40), \\\\n\\\\t\"State\" NVARCHAR(40), \\\\n\\\\t\"Country\" NVARCHAR(40), \\\\n\\\\t\"PostalCode\" NVARCHAR(10), \\\\n\\\\t\"Phone\" NVARCHAR(24), \\\\n\\\\t\"Fax\" NVARCHAR(24), \\\\n\\\\t\"Email\" NVARCHAR(60), \\\\n\\\\tPRIMARY KEY (\"EmployeeId\"), \\\\n\\\\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Employee table:\\\\nEmployeeId\\\\tLastName\\\\tFirstName\\\\tTitle\\\\tReportsTo\\\\tBirthDate\\\\tHireDate\\\\tAddress\\\\tCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\n1\\\\tAdams\\\\tAndrew\\\\tGeneral Manager\\\\tNone\\\\t1962-02-18 00:00:00\\\\t2002-08-14 00:00:00\\\\t11120 Jasper Ave NW\\\\tEdmonton\\\\tAB\\\\tCanada\\\\tT5K 2N1\\\\t+1 (780) 428-9482\\\\t+1 (780) 428-3457\\\\tandrew@chinookcorp.com\\\\n2\\\\tEdwards\\\\tNancy\\\\tSales Manager\\\\t1\\\\t1958-12-08 00:00:00\\\\t2002-05-01 00:00:00\\\\t825 8 Ave SW\\\\tCalgary\\\\tAB\\\\tCanada\\\\tT2P 2T3\\\\t+1 (403) 262-3443\\\\t+1 (403) 262-3322\\\\tnancy@chinookcorp.com\\\\n3\\\\tPeacock\\\\tJane\\\\tSales Support Agent\\\\t2\\\\t1973-08-29 00:00:00\\\\t2002-04-01 00:00:00\\\\t1111 6 Ave SW\\\\tCalgary\\\\tAB\\\\tCanada\\\\tT2P 5M5\\\\t+1 (403) 262-3443\\\\t+1 (403) 262-6712\\\\tjane@chinookcorp.com\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Genre\" (\\\\n\\\\t\"GenreId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"GenreId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Genre table:\\\\nGenreId\\\\tName\\\\n1\\\\tRock\\\\n2\\\\tJazz\\\\n3\\\\tMetal\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"MediaType\" (\\\\n\\\\t\"MediaTypeId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"MediaTypeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from MediaType table:\\\\nMediaTypeId\\\\tName\\\\n1\\\\tMPEG audio file\\\\n2\\\\tProtected AAC audio file\\\\n3\\\\tProtected MPEG-4 video file\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Playlist\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Playlist table:\\\\nPlaylistId\\\\tName\\\\n1\\\\tMusic\\\\n2\\\\tMovies\\\\n3\\\\tTV Shows\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Album\" (\\\\n\\\\t\"AlbumId\" INTEGER NOT NULL, \\\\n\\\\t\"Title\" NVARCHAR(160) NOT NULL, \\\\n\\\\t\"ArtistId\" INTEGER NOT NULL, \\\\n\\\\tPRIMARY KEY (\"AlbumId\"), \\\\n\\\\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Album table:\\\\nAlbumId\\\\tTitle\\\\tArtistId\\\\n1\\\\tFor Those About To Rock We Salute You\\\\t1\\\\n2\\\\tBalls to the Wall\\\\t2\\\\n3\\\\tRestless and Wild\\\\t2\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Customer\" (\\\\n\\\\t\"CustomerId\" INTEGER NOT NULL, \\\\n\\\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\\\n\\\\t\"LastName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"Company\" NVARCHAR(80), \\\\n\\\\t\"Address\" NVARCHAR(70), \\\\n\\\\t\"City\" NVARCHAR(40), \\\\n\\\\t\"State\" NVARCHAR(40), \\\\n\\\\t\"Country\" NVARCHAR(40), \\\\n\\\\t\"PostalCode\" NVARCHAR(10), \\\\n\\\\t\"Phone\" NVARCHAR(24), \\\\n\\\\t\"Fax\" NVARCHAR(24), \\\\n\\\\t\"Email\" NVARCHAR(60) NOT NULL, \\\\n\\\\t\"SupportRepId\" INTEGER, \\\\n\\\\tPRIMARY KEY (\"CustomerId\"), \\\\n\\\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Customer table:\\\\nCustomerId\\\\tFirstName\\\\tLastName\\\\tCompany\\\\tAddress\\\\tCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\tSupportRepId\\\\n1\\\\tLuís\\\\tGonçalves\\\\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\\\\tAv. Brigadeiro Faria Lima, 2170\\\\tSão José dos Campos\\\\tSP\\\\tBrazil\\\\t12227-000\\\\t+55 (12) 3923-5555\\\\t+55 (12) 3923-5566\\\\tluisg@embraer.com.br\\\\t3\\\\n2\\\\tLeonie\\\\tKöhler\\\\tNone\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\tNone\\\\tGermany\\\\t70174\\\\t+49 0711 2842222\\\\tNone\\\\tleonekohler@surfeu.de\\\\t5\\\\n3\\\\tFrançois\\\\tTremblay\\\\tNone\\\\t1498 rue Bélanger\\\\tMontréal\\\\tQC\\\\tCanada\\\\tH2G 1A7\\\\t+1 (514) 721-4711\\\\tNone\\\\tftremblay@gmail.com\\\\t3\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Invoice\" (\\\\n\\\\t\"InvoiceId\" INTEGER NOT NULL, \\\\n\\\\t\"CustomerId\" INTEGER NOT NULL, \\\\n\\\\t\"InvoiceDate\" DATETIME NOT NULL, \\\\n\\\\t\"BillingAddress\" NVARCHAR(70), \\\\n\\\\t\"BillingCity\" NVARCHAR(40), \\\\n\\\\t\"BillingState\" NVARCHAR(40), \\\\n\\\\t\"BillingCountry\" NVARCHAR(40), \\\\n\\\\t\"BillingPostalCode\" NVARCHAR(10), \\\\n\\\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\\\n\\\\tPRIMARY KEY (\"InvoiceId\"), \\\\n\\\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Invoice table:\\\\nInvoiceId\\\\tCustomerId\\\\tInvoiceDate\\\\tBillingAddress\\\\tBillingCity\\\\tBillingState\\\\tBillingCountry\\\\tBillingPostalCode\\\\tTotal\\\\n1\\\\t2\\\\t2009-01-01 00:00:00\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\tNone\\\\tGermany\\\\t70174\\\\t1.98\\\\n2\\\\t4\\\\t2009-01-02 00:00:00\\\\tUllevålsveien 14\\\\tOslo\\\\tNone\\\\tNorway\\\\t0171\\\\t3.96\\\\n3\\\\t8\\\\t2009-01-03 00:00:00\\\\tGrétrystraat 63\\\\tBrussels\\\\tNone\\\\tBelgium\\\\t1000\\\\t5.94\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Track\" (\\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(200) NOT NULL, \\\\n\\\\t\"AlbumId\" INTEGER, \\\\n\\\\t\"MediaTypeId\" INTEGER NOT NULL, \\\\n\\\\t\"GenreId\" INTEGER, \\\\n\\\\t\"Composer\" NVARCHAR(220), \\\\n\\\\t\"Milliseconds\" INTEGER NOT NULL, \\\\n\\\\t\"Bytes\" INTEGER, \\\\n\\\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\\\n\\\\tPRIMARY KEY (\"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \\\\n\\\\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \\\\n\\\\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Track table:\\\\nTrackId\\\\tName\\\\tAlbumId\\\\tMediaTypeId\\\\tGenreId\\\\tComposer\\\\tMilliseconds\\\\tBytes\\\\tUnitPrice\\\\n1\\\\tFor Those About To Rock (We Salute You)\\\\t1\\\\t1\\\\t1\\\\tAngus Young, Malcolm Young, Brian Johnson\\\\t343719\\\\t11170334\\\\t0.99\\\\n2\\\\tBalls to the Wall\\\\t2\\\\t2\\\\t1\\\\tNone\\\\t342562\\\\t5510424\\\\t0.99\\\\n3\\\\tFast As a Shark\\\\t3\\\\t2\\\\t1\\\\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\\\\t230619\\\\t3990994\\\\t0.99\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"InvoiceLine\" (\\\\n\\\\t\"InvoiceLineId\" INTEGER NOT NULL, \\\\n\\\\t\"InvoiceId\" INTEGER NOT NULL, \\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\\\n\\\\t\"Quantity\" INTEGER NOT NULL, \\\\n\\\\tPRIMARY KEY (\"InvoiceLineId\"), \\\\n\\\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from InvoiceLine table:\\\\nInvoiceLineId\\\\tInvoiceId\\\\tTrackId\\\\tUnitPrice\\\\tQuantity\\\\n1\\\\t1\\\\t2\\\\t0.99\\\\t1\\\\n2\\\\t1\\\\t4\\\\t0.99\\\\t1\\\\n3\\\\t2\\\\t6\\\\t0.99\\\\t1\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"PlaylistTrack\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from PlaylistTrack table:\\\\nPlaylistId\\\\tTrackId\\\\n1\\\\t3402\\\\n1\\\\t3389\\\\n1\\\\t3390\\\\n*/\\',\\n      \\'stop\\': [\\'\\\\nSQLResult:\\']},\\n     \\'SELECT COUNT(*) FROM Employee;\\',\\n     {\\'query\\': \\'SELECT COUNT(*) FROM Employee;\\', \\'dialect\\': \\'sqlite\\'},\\n     \\'SELECT COUNT(*) FROM Employee;\\',\\n     \\'[(8,)]\\']\\n```\\n\\n</CodeOutputBlock>\\n\\n## Adding Memory\\n\\nHow to add memory to a SQLDatabaseChain:\\n\\n```python\\nfrom langchain_openai import OpenAI\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_experimental.sql import SQLDatabaseChain\\n```\\n\\nSet up the SQLDatabase and LLM\\n\\n```python\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\nllm = OpenAI(temperature=0, verbose=True)\\n```\\n\\nSet up the memory\\n\\n```python\\nfrom langchain.memory import ConversationBufferMemory\\nmemory = ConversationBufferMemory()\\n```\\n\\nNow we need to add a place for memory in the prompt template\\n\\n```python\\nfrom langchain.prompts import PromptTemplate\\nPROMPT_SUFFIX = \"\"\"Only use the following tables:\\n{table_info}\\n\\nPrevious Conversation:\\n{history}\\n\\nQuestion: {input}\"\"\"\\n\\n_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\\n\\nNever query for all the columns from a specific table, only ask for a few relevant columns given the question.\\n\\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nPROMPT = PromptTemplate.from_template(\\n    _DEFAULT_TEMPLATE + PROMPT_SUFFIX,\\n)\\n```\\n\\nNow let\\'s create and run out chain\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True, memory=memory)\\ndb_chain.run(\"name one employee\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    > Entering new SQLDatabaseChain chain...\\n    name one employee\\n    SQLQuery:SELECT FirstName, LastName FROM Employee LIMIT 1\\n    SQLResult: [(\\'Andrew\\', \\'Adams\\')]\\n    Answer:Andrew Adams\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    \\'Andrew Adams\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n```python\\ndb_chain.run(\"how many letters in their name?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    > Entering new SQLDatabaseChain chain...\\n    how many letters in their name?\\n    SQLQuery:SELECT LENGTH(FirstName) + LENGTH(LastName) AS \\'NameLength\\' FROM Employee WHERE FirstName = \\'Andrew\\' AND LastName = \\'Adams\\'\\n    SQLResult: [(11,)]\\n    Answer:Andrew Adams has 11 letters in their name.\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    \\'Andrew Adams has 11 letters in their name.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n## Choosing how to limit the number of rows returned\\nIf you are querying for several rows of a table you can select the maximum number of results you want to get by using the \\'top_k\\' parameter (default is 10). This is useful for avoiding query results that exceed the prompt max length or consume tokens unnecessarily.\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True, top_k=3)\\n```\\n\\n\\n```python\\ndb_chain.run(\"What are some example tracks by composer Johann Sebastian Bach?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    What are some example tracks by composer Johann Sebastian Bach?\\n    SQLQuery:SELECT Name FROM Track WHERE Composer = \\'Johann Sebastian Bach\\' LIMIT 3\\n    SQLResult: [(\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',)]\\n    Answer:Examples of tracks by Johann Sebastian Bach are Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace, Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria, and Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude.\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    \\'Examples of tracks by Johann Sebastian Bach are Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace, Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria, and Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n## Adding example rows from each table\\nSometimes, the format of the data is not obvious and it is optimal to include a sample of rows from the tables in the prompt to allow the LLM to understand the data before providing a final query. Here we will use this feature to let the LLM know that artists are saved with their full names by providing two rows from the `Track` table.\\n\\n\\n```python\\ndb = SQLDatabase.from_uri(\\n    \"sqlite:///../../../../notebooks/Chinook.db\",\\n    include_tables=[\\'Track\\'], # we include only one table to save tokens in the prompt :)\\n    sample_rows_in_table_info=2)\\n```\\n\\nThe sample rows are added to the prompt after each corresponding table\\'s column information:\\n\\n\\n```python\\nprint(db.table_info)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n    CREATE TABLE \"Track\" (\\n    \\t\"TrackId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(200) NOT NULL,\\n    \\t\"AlbumId\" INTEGER,\\n    \\t\"MediaTypeId\" INTEGER NOT NULL,\\n    \\t\"GenreId\" INTEGER,\\n    \\t\"Composer\" NVARCHAR(220),\\n    \\t\"Milliseconds\" INTEGER NOT NULL,\\n    \\t\"Bytes\" INTEGER,\\n    \\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\\n    \\tPRIMARY KEY (\"TrackId\"),\\n    \\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\\n    \\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\\n    \\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\n    )\\n\\n    /*\\n    2 rows from Track table:\\n    TrackId\\tName\\tAlbumId\\tMediaTypeId\\tGenreId\\tComposer\\tMilliseconds\\tBytes\\tUnitPrice\\n    1\\tFor Those About To Rock (We Salute You)\\t1\\t1\\t1\\tAngus Young, Malcolm Young, Brian Johnson\\t343719\\t11170334\\t0.99\\n    2\\tBalls to the Wall\\t2\\t2\\t1\\tNone\\t342562\\t5510424\\t0.99\\n    */\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, use_query_checker=True, verbose=True)\\n```\\n\\n\\n```python\\ndb_chain.run(\"What are some example tracks by Bach?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    What are some example tracks by Bach?\\n    SQLQuery:SELECT \"Name\", \"Composer\" FROM \"Track\" WHERE \"Composer\" LIKE \\'%Bach%\\' LIMIT 5\\n    SQLResult: [(\\'American Woman\\', \\'B. Cummings/G. Peterson/M.J. Kale/R. Bachman\\'), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Johann Sebastian Bach\\'), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Johann Sebastian Bach\\'), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', \\'Johann Sebastian Bach\\'), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\', \\'Johann Sebastian Bach\\')]\\n    Answer:Tracks by Bach include \\'American Woman\\', \\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', and \\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\'.\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    \\'Tracks by Bach include \\\\\\'American Woman\\\\\\', \\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\', \\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\', \\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\', and \\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\'.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n### Custom Table Info\\nIn some cases, it can be useful to provide custom table information instead of using the automatically generated table definitions and the first `sample_rows_in_table_info` sample rows. For example, if you know that the first few rows of a table are uninformative, it could help to manually provide example rows that are more diverse or provide more information to the model. It is also possible to limit the columns that will be visible to the model if there are unnecessary columns.\\n\\nThis information can be provided as a dictionary with table names as the keys and table information as the values. For example, let\\'s provide a custom definition and sample rows for the Track table with only a few columns:\\n\\n\\n```python\\ncustom_table_info = {\\n    \"Track\": \"\"\"CREATE TABLE Track (\\n\\t\"TrackId\" INTEGER NOT NULL,\\n\\t\"Name\" NVARCHAR(200) NOT NULL,\\n\\t\"Composer\" NVARCHAR(220),\\n\\tPRIMARY KEY (\"TrackId\")\\n)\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tComposer\\n1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n2\\tBalls to the Wall\\tNone\\n3\\tMy favorite song ever\\tThe coolest composer of all time\\n*/\"\"\"\\n}\\n```\\n\\n\\n```python\\ndb = SQLDatabase.from_uri(\\n    \"sqlite:///../../../../notebooks/Chinook.db\",\\n    include_tables=[\\'Track\\', \\'Playlist\\'],\\n    sample_rows_in_table_info=2,\\n    custom_table_info=custom_table_info)\\n\\nprint(db.table_info)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n    CREATE TABLE \"Playlist\" (\\n    \\t\"PlaylistId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(120),\\n    \\tPRIMARY KEY (\"PlaylistId\")\\n    )\\n\\n    /*\\n    2 rows from Playlist table:\\n    PlaylistId\\tName\\n    1\\tMusic\\n    2\\tMovies\\n    */\\n\\n    CREATE TABLE Track (\\n    \\t\"TrackId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(200) NOT NULL,\\n    \\t\"Composer\" NVARCHAR(220),\\n    \\tPRIMARY KEY (\"TrackId\")\\n    )\\n    /*\\n    3 rows from Track table:\\n    TrackId\\tName\\tComposer\\n    1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n    2\\tBalls to the Wall\\tNone\\n    3\\tMy favorite song ever\\tThe coolest composer of all time\\n    */\\n```\\n\\n</CodeOutputBlock>\\n\\nNote how our custom table definition and sample rows for `Track` overrides the `sample_rows_in_table_info` parameter. Tables that are not overridden by `custom_table_info`, in this example `Playlist`, will have their table info gathered automatically as usual.\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\\ndb_chain.run(\"What are some example tracks by Bach?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    What are some example tracks by Bach?\\n    SQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\'%Bach%\\' LIMIT 5;\\n    SQLResult: [(\\'American Woman\\',), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\',)]\\n    Answer:text=\\'You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\\\n\\\\nUse the following format:\\\\n\\\\nQuestion: \"Question here\"\\\\nSQLQuery: \"SQL Query to run\"\\\\nSQLResult: \"Result of the SQLQuery\"\\\\nAnswer: \"Final answer here\"\\\\n\\\\nOnly use the following tables:\\\\n\\\\nCREATE TABLE \"Playlist\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n2 rows from Playlist table:\\\\nPlaylistId\\\\tName\\\\n1\\\\tMusic\\\\n2\\\\tMovies\\\\n*/\\\\n\\\\nCREATE TABLE Track (\\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(200) NOT NULL,\\\\n\\\\t\"Composer\" NVARCHAR(220),\\\\n\\\\tPRIMARY KEY (\"TrackId\")\\\\n)\\\\n/*\\\\n3 rows from Track table:\\\\nTrackId\\\\tName\\\\tComposer\\\\n1\\\\tFor Those About To Rock (We Salute You)\\\\tAngus Young, Malcolm Young, Brian Johnson\\\\n2\\\\tBalls to the Wall\\\\tNone\\\\n3\\\\tMy favorite song ever\\\\tThe coolest composer of all time\\\\n*/\\\\n\\\\nQuestion: What are some example tracks by Bach?\\\\nSQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\\\\\'%Bach%\\\\\\' LIMIT 5;\\\\nSQLResult: [(\\\\\\'American Woman\\\\\\',), (\\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\',), (\\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\',), (\\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\',), (\\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\',)]\\\\nAnswer:\\'\\n    You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\n    Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\n    Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\n    Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\n\\n    Use the following format:\\n\\n    Question: \"Question here\"\\n    SQLQuery: \"SQL Query to run\"\\n    SQLResult: \"Result of the SQLQuery\"\\n    Answer: \"Final answer here\"\\n\\n    Only use the following tables:\\n\\n    CREATE TABLE \"Playlist\" (\\n    \\t\"PlaylistId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(120),\\n    \\tPRIMARY KEY (\"PlaylistId\")\\n    )\\n\\n    /*\\n    2 rows from Playlist table:\\n    PlaylistId\\tName\\n    1\\tMusic\\n    2\\tMovies\\n    */\\n\\n    CREATE TABLE Track (\\n    \\t\"TrackId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(200) NOT NULL,\\n    \\t\"Composer\" NVARCHAR(220),\\n    \\tPRIMARY KEY (\"TrackId\")\\n    )\\n    /*\\n    3 rows from Track table:\\n    TrackId\\tName\\tComposer\\n    1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n    2\\tBalls to the Wall\\tNone\\n    3\\tMy favorite song ever\\tThe coolest composer of all time\\n    */\\n\\n    Question: What are some example tracks by Bach?\\n    SQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\'%Bach%\\' LIMIT 5;\\n    SQLResult: [(\\'American Woman\\',), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\',)]\\n    Answer:\\n    {\\'input\\': \\'What are some example tracks by Bach?\\\\nSQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\\\\\'%Bach%\\\\\\' LIMIT 5;\\\\nSQLResult: [(\\\\\\'American Woman\\\\\\',), (\\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\',), (\\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\',), (\\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\',), (\\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\',)]\\\\nAnswer:\\', \\'top_k\\': \\'5\\', \\'dialect\\': \\'sqlite\\', \\'table_info\\': \\'\\\\nCREATE TABLE \"Playlist\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n2 rows from Playlist table:\\\\nPlaylistId\\\\tName\\\\n1\\\\tMusic\\\\n2\\\\tMovies\\\\n*/\\\\n\\\\nCREATE TABLE Track (\\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(200) NOT NULL,\\\\n\\\\t\"Composer\" NVARCHAR(220),\\\\n\\\\tPRIMARY KEY (\"TrackId\")\\\\n)\\\\n/*\\\\n3 rows from Track table:\\\\nTrackId\\\\tName\\\\tComposer\\\\n1\\\\tFor Those About To Rock (We Salute You)\\\\tAngus Young, Malcolm Young, Brian Johnson\\\\n2\\\\tBalls to the Wall\\\\tNone\\\\n3\\\\tMy favorite song ever\\\\tThe coolest composer of all time\\\\n*/\\', \\'stop\\': [\\'\\\\nSQLResult:\\']}\\n    Examples of tracks by Bach include \"American Woman\", \"Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\", \"Aria Mit 30 Veränderungen, BWV 988 \\'Goldberg Variations\\': Aria\", \"Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\", and \"Toccata and Fugue in D Minor, BWV 565: I. Toccata\".\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    \\'Examples of tracks by Bach include \"American Woman\", \"Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\", \"Aria Mit 30 Veränderungen, BWV 988 \\\\\\'Goldberg Variations\\\\\\': Aria\", \"Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\", and \"Toccata and Fugue in D Minor, BWV 565: I. Toccata\".\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n### SQL Views\\n\\nIn some case, the table schema can be hidden behind a JSON or JSONB column. Adding row samples into the prompt might help won\\'t always describe the data perfectly.\\n\\nFor this reason, a custom SQL views can help.\\n\\n```sql\\nCREATE VIEW accounts_v AS\\n    select id, firstname, lastname, email, created_at, updated_at,\\n        cast(stats->>\\'total_post\\' as int) as total_post,\\n        cast(stats->>\\'total_comments\\' as int) as total_comments,\\n        cast(stats->>\\'ltv\\' as int) as ltv\\n\\n        FROM accounts;\\n```\\n\\nThen limit the tables visible from SQLDatabase to the created view.\\n\\n```python\\ndb = SQLDatabase.from_uri(\\n    \"sqlite:///../../../../notebooks/Chinook.db\",\\n    include_tables=[\\'accounts_v\\']) # we include only the view\\n```\\n\\n## SQLDatabaseSequentialChain\\n\\nChain for querying SQL database that is a sequential chain.\\n\\nThe chain is as follows:\\n\\n    1. Based on the query, determine which tables to use.\\n    2. Based on those tables, call the normal SQL database chain.\\n\\nThis is useful in cases where the number of tables in the database is large.\\n\\n\\n```python\\nfrom langchain_experimental.sql import SQLDatabaseSequentialChain\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\n```\\n\\n\\n```python\\nchain = SQLDatabaseSequentialChain.from_llm(llm, db, verbose=True)\\n```\\n\\n\\n```python\\nchain.run(\"How many employees are also customers?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseSequentialChain chain...\\n    Table names to use:\\n    [\\'Employee\\', \\'Customer\\']\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many employees are also customers?\\n    SQLQuery:SELECT COUNT(*) FROM Employee e INNER JOIN Customer c ON e.EmployeeId = c.SupportRepId;\\n    SQLResult: [(59,)]\\n    Answer:59 employees are also customers.\\n    > Finished chain.\\n\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    \\'59 employees are also customers.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n## Using Local Language Models\\n\\n\\nSometimes you may not have the luxury of using OpenAI or other service-hosted large language model. You can, ofcourse, try to use the `SQLDatabaseChain` with a local model, but will quickly realize that most models you can run locally even with a large GPU struggle to generate the right output.\\n\\n\\n```python\\nimport logging\\nimport torch\\nfrom transformers import AutoTokenizer, GPT2TokenizerFast, pipeline, AutoModelForSeq2SeqLM, AutoModelForCausalLM\\nfrom langchain_huggingface import HuggingFacePipeline\\n\\n# Note: This model requires a large GPU, e.g. an 80GB A100. See documentation for other ways to run private non-OpenAI models.\\nmodel_id = \"google/flan-ul2\"\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, temperature=0)\\n\\ndevice_id = -1  # default to no-GPU, but use GPU and half precision mode if available\\nif torch.cuda.is_available():\\n    device_id = 0\\n    try:\\n        model = model.half()\\n    except RuntimeError as exc:\\n        logging.warn(f\"Could not run model in half precision mode: {str(exc)}\")\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\npipe = pipeline(task=\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=1024, device=device_id)\\n\\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    Loading checkpoint shards: 100%|██████████| 8/8 [00:32<00:00,  4.11s/it]\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_experimental.sql import SQLDatabaseChain\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\", include_tables=[\\'Customer\\'])\\nlocal_chain = SQLDatabaseChain.from_llm(local_llm, db, verbose=True, return_intermediate_steps=True, use_query_checker=True)\\n```\\n\\nThis model should work for very simple SQL queries, as long as you use the query checker as specified above, e.g.:\\n\\n\\n```python\\nlocal_chain(\"How many customers are there?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many customers are there?\\n    SQLQuery:\\n\\n    /workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn(\\n    /workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn(\\n\\n\\n    SELECT count(*) FROM Customer\\n    SQLResult: [(59,)]\\n    Answer:\\n\\n    /workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn(\\n\\n\\n    [59]\\n    > Finished chain.\\n\\n\\n\\n\\n\\n    {\\'query\\': \\'How many customers are there?\\',\\n     \\'result\\': \\'[59]\\',\\n     \\'intermediate_steps\\': [{\\'input\\': \\'How many customers are there?\\\\nSQLQuery:SELECT count(*) FROM Customer\\\\nSQLResult: [(59,)]\\\\nAnswer:\\',\\n       \\'top_k\\': \\'5\\',\\n       \\'dialect\\': \\'sqlite\\',\\n       \\'table_info\\': \\'\\\\nCREATE TABLE \"Customer\" (\\\\n\\\\t\"CustomerId\" INTEGER NOT NULL, \\\\n\\\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\\\n\\\\t\"LastName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"Company\" NVARCHAR(80), \\\\n\\\\t\"Address\" NVARCHAR(70), \\\\n\\\\t\"City\" NVARCHAR(40), \\\\n\\\\t\"State\" NVARCHAR(40), \\\\n\\\\t\"Country\" NVARCHAR(40), \\\\n\\\\t\"PostalCode\" NVARCHAR(10), \\\\n\\\\t\"Phone\" NVARCHAR(24), \\\\n\\\\t\"Fax\" NVARCHAR(24), \\\\n\\\\t\"Email\" NVARCHAR(60) NOT NULL, \\\\n\\\\t\"SupportRepId\" INTEGER, \\\\n\\\\tPRIMARY KEY (\"CustomerId\"), \\\\n\\\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Customer table:\\\\nCustomerId\\\\tFirstName\\\\tLastName\\\\tCompany\\\\tAddress\\\\tCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\tSupportRepId\\\\n1\\\\tLuís\\\\tGonçalves\\\\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\\\\tAv. Brigadeiro Faria Lima, 2170\\\\tSão José dos Campos\\\\tSP\\\\tBrazil\\\\t12227-000\\\\t+55 (12) 3923-5555\\\\t+55 (12) 3923-5566\\\\tluisg@embraer.com.br\\\\t3\\\\n2\\\\tLeonie\\\\tKöhler\\\\tNone\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\tNone\\\\tGermany\\\\t70174\\\\t+49 0711 2842222\\\\tNone\\\\tleonekohler@surfeu.de\\\\t5\\\\n3\\\\tFrançois\\\\tTremblay\\\\tNone\\\\t1498 rue Bélanger\\\\tMontréal\\\\tQC\\\\tCanada\\\\tH2G 1A7\\\\t+1 (514) 721-4711\\\\tNone\\\\tftremblay@gmail.com\\\\t3\\\\n*/\\',\\n       \\'stop\\': [\\'\\\\nSQLResult:\\']},\\n      \\'SELECT count(*) FROM Customer\\',\\n      {\\'query\\': \\'SELECT count(*) FROM Customer\\', \\'dialect\\': \\'sqlite\\'},\\n      \\'SELECT count(*) FROM Customer\\',\\n      \\'[(59,)]\\']}\\n```\\n\\n</CodeOutputBlock>\\n\\nEven this relatively large model will most likely fail to generate more complicated SQL by itself. However, you can log its inputs and outputs so that you can hand-correct them and use the corrected examples for few-shot prompt examples later. In practice, you could log any executions of your chain that raise exceptions (as shown in the example below) or get direct user feedback in cases where the results are incorrect (but did not raise an exception).\\n\\n\\n```bash\\npoetry run pip install pyyaml langchain_chroma\\nimport yaml\\n```\\n\\n<CodeOutputBlock lang=\"bash\">\\n\\n```\\n    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\\n    To disable this warning, you can either:\\n    \\t- Avoid using `tokenizers` before the fork if possible\\n    \\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\\n\\n\\n    11842.36s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\\n\\n\\n    Requirement already satisfied: pyyaml in /workspace/langchain/.venv/lib/python3.9/site-packages (6.0)\\n    Requirement already satisfied: chromadb in /workspace/langchain/.venv/lib/python3.9/site-packages (0.3.21)\\n    Requirement already satisfied: pandas>=1.3 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (2.0.1)\\n    Requirement already satisfied: requests>=2.28 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (2.28.2)\\n    Requirement already satisfied: pydantic>=1.9 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.10.7)\\n    Requirement already satisfied: hnswlib>=0.7 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.0)\\n    Requirement already satisfied: clickhouse-connect>=0.5.7 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.5.20)\\n    Requirement already satisfied: sentence-transformers>=2.2.2 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (2.2.2)\\n    Requirement already satisfied: duckdb>=0.7.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.1)\\n    Requirement already satisfied: fastapi>=0.85.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.95.1)\\n    Requirement already satisfied: uvicorn[standard]>=0.18.3 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.21.1)\\n    Requirement already satisfied: numpy>=1.21.6 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.24.3)\\n    Requirement already satisfied: posthog>=2.4.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.0.1)\\n    Requirement already satisfied: certifi in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\\n    Requirement already satisfied: urllib3>=1.26 in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\\n    Requirement already satisfied: pytz in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3)\\n    Requirement already satisfied: zstandard in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\\n    Requirement already satisfied: lz4 in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\\n    Requirement already satisfied: starlette<0.27.0,>=0.26.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from fastapi>=0.85.1->chromadb) (0.26.1)\\n    Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/langchain/.venv/lib/python3.9/site-packages (from pandas>=1.3->chromadb) (2.8.2)\\n    Requirement already satisfied: tzdata>=2022.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from pandas>=1.3->chromadb) (2023.3)\\n    Requirement already satisfied: six>=1.5 in /workspace/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\\n    Requirement already satisfied: monotonic>=1.5 in /workspace/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6)\\n    Requirement already satisfied: backoff>=1.10.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\\n    Requirement already satisfied: typing-extensions>=4.2.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from pydantic>=1.9->chromadb) (4.5.0)\\n    Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/langchain/.venv/lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.1.0)\\n    Requirement already satisfied: idna<4,>=2.5 in /workspace/langchain/.venv/lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.4)\\n    Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.28.1)\\n    Requirement already satisfied: tqdm in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.65.0)\\n    Requirement already satisfied: torch>=1.6.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.13.1)\\n    Requirement already satisfied: torchvision in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.14.1)\\n    Requirement already satisfied: scikit-learn in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.2.2)\\n    Requirement already satisfied: scipy in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.9.3)\\n    Requirement already satisfied: nltk in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (3.8.1)\\n    Requirement already satisfied: sentencepiece in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.1.98)\\n    Requirement already satisfied: huggingface-hub>=0.4.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.13.4)\\n    Requirement already satisfied: click>=7.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\\n    Requirement already satisfied: h11>=0.8 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\\n    Requirement already satisfied: httptools>=0.5.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.5.0)\\n    Requirement already satisfied: python-dotenv>=0.13 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\\n    Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\\n    Requirement already satisfied: watchfiles>=0.13 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\\n    Requirement already satisfied: websockets>=10.4 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.2)\\n    Requirement already satisfied: filelock in /workspace/langchain/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (3.12.0)\\n    Requirement already satisfied: packaging>=20.9 in /workspace/langchain/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (23.1)\\n    Requirement already satisfied: anyio<5,>=3.4.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (3.6.2)\\n    Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (11.7.99)\\n    Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (8.5.0.96)\\n    Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (11.10.3.66)\\n    Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (11.7.99)\\n    Requirement already satisfied: setuptools in /workspace/langchain/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (67.7.1)\\n    Requirement already satisfied: wheel in /workspace/langchain/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (0.40.0)\\n    Requirement already satisfied: regex!=2019.12.17 in /workspace/langchain/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (2023.3.23)\\n    Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (0.13.3)\\n    Requirement already satisfied: joblib in /workspace/langchain/.venv/lib/python3.9/site-packages (from nltk->sentence-transformers>=2.2.2->chromadb) (1.2.0)\\n    Requirement already satisfied: threadpoolctl>=2.0.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb) (3.1.0)\\n    Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torchvision->sentence-transformers>=2.2.2->chromadb) (9.5.0)\\n    Requirement already satisfied: sniffio>=1.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (1.3.0)\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nfrom typing import Dict\\n\\nQUERY = \"List all the customer first names that start with \\'a\\'\"\\n\\ndef _parse_example(result: Dict) -> Dict:\\n    sql_cmd_key = \"sql_cmd\"\\n    sql_result_key = \"sql_result\"\\n    table_info_key = \"table_info\"\\n    input_key = \"input\"\\n    final_answer_key = \"answer\"\\n\\n    _example = {\\n        \"input\": result.get(\"query\"),\\n    }\\n\\n    steps = result.get(\"intermediate_steps\")\\n    answer_key = sql_cmd_key # the first one\\n    for step in steps:\\n        # The steps are in pairs, a dict (input) followed by a string (output).\\n        # Unfortunately there is no schema but you can look at the input key of the\\n        # dict to see what the output is supposed to be\\n        if isinstance(step, dict):\\n            # Grab the table info from input dicts in the intermediate steps once\\n            if table_info_key not in _example:\\n                _example[table_info_key] = step.get(table_info_key)\\n\\n            if input_key in step:\\n                if step[input_key].endswith(\"SQLQuery:\"):\\n                    answer_key = sql_cmd_key # this is the SQL generation input\\n                if step[input_key].endswith(\"Answer:\"):\\n                    answer_key = final_answer_key # this is the final answer input\\n            elif sql_cmd_key in step:\\n                _example[sql_cmd_key] = step[sql_cmd_key]\\n                answer_key = sql_result_key # this is SQL execution input\\n        elif isinstance(step, str):\\n            # The preceding element should have set the answer_key\\n            _example[answer_key] = step\\n    return _example\\n\\nexample: any\\ntry:\\n    result = local_chain(QUERY)\\n    print(\"*** Query succeeded\")\\n    example = _parse_example(result)\\nexcept Exception as exc:\\n    print(\"*** Query failed\")\\n    result = {\\n        \"query\": QUERY,\\n        \"intermediate_steps\": exc.intermediate_steps\\n    }\\n    example = _parse_example(result)\\n\\n\\n# print for now, in reality you may want to write this out to a YAML file or database for manual fix-ups offline\\nyaml_example = yaml.dump(example, allow_unicode=True)\\nprint(\"\\\\n\" + yaml_example)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    List all the customer first names that start with \\'a\\'\\n    SQLQuery:\\n\\n    /workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn(\\n\\n\\n    SELECT firstname FROM customer WHERE firstname LIKE \\'%a%\\'\\n    SQLResult: [(\\'François\\',), (\\'František\\',), (\\'Helena\\',), (\\'Astrid\\',), (\\'Daan\\',), (\\'Kara\\',), (\\'Eduardo\\',), (\\'Alexandre\\',), (\\'Fernanda\\',), (\\'Mark\\',), (\\'Frank\\',), (\\'Jack\\',), (\\'Dan\\',), (\\'Kathy\\',), (\\'Heather\\',), (\\'Frank\\',), (\\'Richard\\',), (\\'Patrick\\',), (\\'Julia\\',), (\\'Edward\\',), (\\'Martha\\',), (\\'Aaron\\',), (\\'Madalena\\',), (\\'Hannah\\',), (\\'Niklas\\',), (\\'Camille\\',), (\\'Marc\\',), (\\'Wyatt\\',), (\\'Isabelle\\',), (\\'Ladislav\\',), (\\'Lucas\\',), (\\'Johannes\\',), (\\'Stanisław\\',), (\\'Joakim\\',), (\\'Emma\\',), (\\'Mark\\',), (\\'Manoj\\',), (\\'Puja\\',)]\\n    Answer:\\n\\n    /workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn(\\n\\n\\n    [(\\'François\\', \\'Frantiek\\', \\'Helena\\', \\'Astrid\\', \\'Daan\\', \\'Kara\\', \\'Eduardo\\', \\'Alexandre\\', \\'Fernanda\\', \\'Mark\\', \\'Frank\\', \\'Jack\\', \\'Dan\\', \\'Kathy\\', \\'Heather\\', \\'Frank\\', \\'Richard\\', \\'Patrick\\', \\'Julia\\', \\'Edward\\', \\'Martha\\', \\'Aaron\\', \\'Madalena\\', \\'Hannah\\', \\'Niklas\\', \\'Camille\\', \\'Marc\\', \\'Wyatt\\', \\'Isabelle\\', \\'Ladislav\\', \\'Lucas\\', \\'Johannes\\', \\'Stanisaw\\', \\'Joakim\\', \\'Emma\\', \\'Mark\\', \\'Manoj\\', \\'Puja\\']\\n    > Finished chain.\\n    *** Query succeeded\\n\\n    answer: \\'[(\\'\\'François\\'\\', \\'\\'Frantiek\\'\\', \\'\\'Helena\\'\\', \\'\\'Astrid\\'\\', \\'\\'Daan\\'\\', \\'\\'Kara\\'\\',\\n      \\'\\'Eduardo\\'\\', \\'\\'Alexandre\\'\\', \\'\\'Fernanda\\'\\', \\'\\'Mark\\'\\', \\'\\'Frank\\'\\', \\'\\'Jack\\'\\', \\'\\'Dan\\'\\',\\n      \\'\\'Kathy\\'\\', \\'\\'Heather\\'\\', \\'\\'Frank\\'\\', \\'\\'Richard\\'\\', \\'\\'Patrick\\'\\', \\'\\'Julia\\'\\', \\'\\'Edward\\'\\',\\n      \\'\\'Martha\\'\\', \\'\\'Aaron\\'\\', \\'\\'Madalena\\'\\', \\'\\'Hannah\\'\\', \\'\\'Niklas\\'\\', \\'\\'Camille\\'\\', \\'\\'Marc\\'\\',\\n      \\'\\'Wyatt\\'\\', \\'\\'Isabelle\\'\\', \\'\\'Ladislav\\'\\', \\'\\'Lucas\\'\\', \\'\\'Johannes\\'\\', \\'\\'Stanisaw\\'\\', \\'\\'Joakim\\'\\',\\n      \\'\\'Emma\\'\\', \\'\\'Mark\\'\\', \\'\\'Manoj\\'\\', \\'\\'Puja\\'\\']\\'\\n    input: List all the customer first names that start with \\'a\\'\\n    sql_cmd: SELECT firstname FROM customer WHERE firstname LIKE \\'%a%\\'\\n    sql_result: \\'[(\\'\\'François\\'\\',), (\\'\\'František\\'\\',), (\\'\\'Helena\\'\\',), (\\'\\'Astrid\\'\\',), (\\'\\'Daan\\'\\',),\\n      (\\'\\'Kara\\'\\',), (\\'\\'Eduardo\\'\\',), (\\'\\'Alexandre\\'\\',), (\\'\\'Fernanda\\'\\',), (\\'\\'Mark\\'\\',), (\\'\\'Frank\\'\\',),\\n      (\\'\\'Jack\\'\\',), (\\'\\'Dan\\'\\',), (\\'\\'Kathy\\'\\',), (\\'\\'Heather\\'\\',), (\\'\\'Frank\\'\\',), (\\'\\'Richard\\'\\',),\\n      (\\'\\'Patrick\\'\\',), (\\'\\'Julia\\'\\',), (\\'\\'Edward\\'\\',), (\\'\\'Martha\\'\\',), (\\'\\'Aaron\\'\\',), (\\'\\'Madalena\\'\\',),\\n      (\\'\\'Hannah\\'\\',), (\\'\\'Niklas\\'\\',), (\\'\\'Camille\\'\\',), (\\'\\'Marc\\'\\',), (\\'\\'Wyatt\\'\\',), (\\'\\'Isabelle\\'\\',),\\n      (\\'\\'Ladislav\\'\\',), (\\'\\'Lucas\\'\\',), (\\'\\'Johannes\\'\\',), (\\'\\'Stanisław\\'\\',), (\\'\\'Joakim\\'\\',),\\n      (\\'\\'Emma\\'\\',), (\\'\\'Mark\\'\\',), (\\'\\'Manoj\\'\\',), (\\'\\'Puja\\'\\',)]\\'\\n    table_info: \"\\\\nCREATE TABLE \\\\\"Customer\\\\\" (\\\\n\\\\t\\\\\"CustomerId\\\\\" INTEGER NOT NULL, \\\\n\\\\t\\\\\\n      \\\\\"FirstName\\\\\" NVARCHAR(40) NOT NULL, \\\\n\\\\t\\\\\"LastName\\\\\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\\\\\\n      \\\\\"Company\\\\\" NVARCHAR(80), \\\\n\\\\t\\\\\"Address\\\\\" NVARCHAR(70), \\\\n\\\\t\\\\\"City\\\\\" NVARCHAR(40),\\\\\\n      \\\\ \\\\n\\\\t\\\\\"State\\\\\" NVARCHAR(40), \\\\n\\\\t\\\\\"Country\\\\\" NVARCHAR(40), \\\\n\\\\t\\\\\"PostalCode\\\\\" NVARCHAR(10),\\\\\\n      \\\\ \\\\n\\\\t\\\\\"Phone\\\\\" NVARCHAR(24), \\\\n\\\\t\\\\\"Fax\\\\\" NVARCHAR(24), \\\\n\\\\t\\\\\"Email\\\\\" NVARCHAR(60)\\\\\\n      \\\\ NOT NULL, \\\\n\\\\t\\\\\"SupportRepId\\\\\" INTEGER, \\\\n\\\\tPRIMARY KEY (\\\\\"CustomerId\\\\\"), \\\\n\\\\t\\\\\\n      FOREIGN KEY(\\\\\"SupportRepId\\\\\") REFERENCES \\\\\"Employee\\\\\" (\\\\\"EmployeeId\\\\\")\\\\n)\\\\n\\\\n/*\\\\n\\\\\\n      3 rows from Customer table:\\\\nCustomerId\\\\tFirstName\\\\tLastName\\\\tCompany\\\\tAddress\\\\t\\\\\\n      City\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\tSupportRepId\\\\n1\\\\tLuís\\\\tGonçalves\\\\t\\\\\\n      Embraer - Empresa Brasileira de Aeronáutica S.A.\\\\tAv. Brigadeiro Faria Lima, 2170\\\\t\\\\\\n      São José dos Campos\\\\tSP\\\\tBrazil\\\\t12227-000\\\\t+55 (12) 3923-5555\\\\t+55 (12) 3923-5566\\\\t\\\\\\n      luisg@embraer.com.br\\\\t3\\\\n2\\\\tLeonie\\\\tKöhler\\\\tNone\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\t\\\\\\n      None\\\\tGermany\\\\t70174\\\\t+49 0711 2842222\\\\tNone\\\\tleonekohler@surfeu.de\\\\t5\\\\n3\\\\tFrançois\\\\t\\\\\\n      Tremblay\\\\tNone\\\\t1498 rue Bélanger\\\\tMontréal\\\\tQC\\\\tCanada\\\\tH2G 1A7\\\\t+1 (514) 721-4711\\\\t\\\\\\n      None\\\\tftremblay@gmail.com\\\\t3\\\\n*/\"\\n\\n```\\n\\n</CodeOutputBlock>\\n\\nRun the snippet above a few times, or log exceptions in your deployed environment, to collect lots of examples of inputs, table_info and sql_cmd generated by your language model. The sql_cmd values will be incorrect and you can manually fix them up to build a collection of examples, e.g. here we are using YAML to keep a neat record of our inputs and corrected SQL output that we can build up over time.\\n\\n\\n```python\\nYAML_EXAMPLES = \"\"\"\\n- input: How many customers are not from Brazil?\\n  table_info: |\\n    CREATE TABLE \"Customer\" (\\n      \"CustomerId\" INTEGER NOT NULL,\\n      \"FirstName\" NVARCHAR(40) NOT NULL,\\n      \"LastName\" NVARCHAR(20) NOT NULL,\\n      \"Company\" NVARCHAR(80),\\n      \"Address\" NVARCHAR(70),\\n      \"City\" NVARCHAR(40),\\n      \"State\" NVARCHAR(40),\\n      \"Country\" NVARCHAR(40),\\n      \"PostalCode\" NVARCHAR(10),\\n      \"Phone\" NVARCHAR(24),\\n      \"Fax\" NVARCHAR(24),\\n      \"Email\" NVARCHAR(60) NOT NULL,\\n      \"SupportRepId\" INTEGER,\\n      PRIMARY KEY (\"CustomerId\"),\\n      FOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\n    )\\n  sql_cmd: SELECT COUNT(*) FROM \"Customer\" WHERE NOT \"Country\" = \"Brazil\";\\n  sql_result: \"[(54,)]\"\\n  answer: 54 customers are not from Brazil.\\n- input: list all the genres that start with \\'r\\'\\n  table_info: |\\n    CREATE TABLE \"Genre\" (\\n      \"GenreId\" INTEGER NOT NULL,\\n      \"Name\" NVARCHAR(120),\\n      PRIMARY KEY (\"GenreId\")\\n    )\\n\\n    /*\\n    3 rows from Genre table:\\n    GenreId\\tName\\n    1\\tRock\\n    2\\tJazz\\n    3\\tMetal\\n    */\\n  sql_cmd: SELECT \"Name\" FROM \"Genre\" WHERE \"Name\" LIKE \\'r%\\';\\n  sql_result: \"[(\\'Rock\\',), (\\'Rock and Roll\\',), (\\'Reggae\\',), (\\'R&B/Soul\\',)]\"\\n  answer: The genres that start with \\'r\\' are Rock, Rock and Roll, Reggae and R&B/Soul.\\n\"\"\"\\n```\\n\\nNow that you have some examples (with manually corrected output SQL), you can do few-shot prompt seeding the usual way:\\n\\n\\n```python\\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\\nfrom langchain.chains.sql_database.prompt import _sqlite_prompt, PROMPT_SUFFIX\\nfrom langchain_huggingface import HuggingFaceEmbeddings\\nfrom langchain.prompts.example_selector.semantic_similarity import SemanticSimilarityExampleSelector\\nfrom langchain_chroma import Chroma\\n\\nexample_prompt = PromptTemplate(\\n    input_variables=[\"table_info\", \"input\", \"sql_cmd\", \"sql_result\", \"answer\"],\\n    template=\"{table_info}\\\\n\\\\nQuestion: {input}\\\\nSQLQuery: {sql_cmd}\\\\nSQLResult: {sql_result}\\\\nAnswer: {answer}\",\\n)\\n\\nexamples_dict = yaml.safe_load(YAML_EXAMPLES)\\n\\nlocal_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n\\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\\n                        # This is the list of examples available to select from.\\n                        examples_dict,\\n                        # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\\n                        local_embeddings,\\n                        # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\\n                        Chroma,  # type: ignore\\n                        # This is the number of examples to produce and include per prompt\\n                        k=min(3, len(examples_dict)),\\n                    )\\n\\nfew_shot_prompt = FewShotPromptTemplate(\\n    example_selector=example_selector,\\n    example_prompt=example_prompt,\\n    prefix=_sqlite_prompt + \"Here are some examples:\",\\n    suffix=PROMPT_SUFFIX,\\n    input_variables=[\"table_info\", \"input\", \"top_k\"],\\n)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    Using embedded DuckDB without persistence: data will be transient\\n```\\n\\n</CodeOutputBlock>\\n\\nThe model should do better now with this few-shot prompt, especially for inputs similar to the examples you have seeded it with.\\n\\n\\n```python\\nlocal_chain = SQLDatabaseChain.from_llm(local_llm, db, prompt=few_shot_prompt, use_query_checker=True, verbose=True, return_intermediate_steps=True)\\n```\\n\\n\\n```python\\nresult = local_chain(\"How many customers are from Brazil?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many customers are from Brazil?\\n    SQLQuery:SELECT count(*) FROM Customer WHERE Country = \"Brazil\";\\n    SQLResult: [(5,)]\\n    Answer:[5]\\n    > Finished chain.\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nresult = local_chain(\"How many customers are not from Brazil?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many customers are not from Brazil?\\n    SQLQuery:SELECT count(*) FROM customer WHERE country NOT IN (SELECT country FROM customer WHERE country = \\'Brazil\\')\\n    SQLResult: [(54,)]\\n    Answer:54 customers are not from Brazil.\\n    > Finished chain.\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nresult = local_chain(\"How many customers are there in total?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many customers are there in total?\\n    SQLQuery:SELECT count(*) FROM Customer;\\n    SQLResult: [(59,)]\\n    Answer:There are 59 customers in total.\\n    > Finished chain.\\n```\\n\\n</CodeOutputBlock>\\n'), Document(id='d2a798b3-e949-4c42-8f2c-6ba0ef04ed6a', metadata={'file_type': '.mdx', 'source': 'docs\\\\docs\\\\people.mdx', 'file_name': 'people.mdx', 'file_path': 'docs\\\\docs\\\\people.mdx'}, page_content='---\\nhide_table_of_contents: true\\n---\\n\\nimport People from \"@theme/People\";\\n\\n# People\\n\\nThere are some incredible humans from all over the world who have been instrumental in helping the LangChain community flourish 🌐!\\n\\nThis page highlights a few of those folks who have dedicated their time to the open-source repo in the form of direct contributions and reviews.\\n\\n## Top reviewers\\n\\nAs LangChain has grown, the amount of surface area that maintainers cover has grown as well.\\n\\nThank you to the following folks who have gone above and beyond in reviewing incoming PRs 🙏!\\n\\n<People type=\"top_reviewers\"></People>\\n\\n## Top recent contributors\\n\\nThe list below contains contributors who have had the most PRs merged in the last three months, weighted (imperfectly) by impact.\\n\\nThank you all so much for your time and efforts in making LangChain better ❤️!\\n\\n<People type=\"top_recent_contributors\" count=\"20\"></People>\\n\\n## Core maintainers\\n\\nHello there 👋!\\n\\nWe\\'re LangChain\\'s core maintainers. If you\\'ve spent time in the community, you\\'ve probably crossed paths\\nwith at least one of us already. \\n\\n<People type=\"maintainers\"></People>\\n\\n## Top all-time contributors\\n\\nAnd finally, this is an all-time list of all-stars who have made significant contributions to the framework 🌟:\\n\\n<People type=\"top_contributors\"></People>\\n\\nWe\\'re so thankful for your support!\\n\\nAnd one more thank you to [@tiangolo](https://github.com/tiangolo) for inspiration via FastAPI\\'s [excellent people page](https://fastapi.tiangolo.com/fastapi-people).\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(output['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ddf115",
   "metadata": {},
   "source": [
    "retrieve 用とリランク用に分けたり、retriever を複数にして LLM にルーティングさせたり、結局マルチエージェントとかTools、MCPっぽい構想なんだな(動的に手札を切らせる)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14359de8",
   "metadata": {},
   "source": [
    "## 複数の Retriever\n",
    "LLM による埋め込みの ベクトル DB 内で距離を測るよりも、TF-IDFなどの出現率ベースのベクトルのほうが専門分野に強かったりする。そのため、Embedding BaseなRetrieverと、TF-IDFベースを組み合わせたりする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90649b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDFも合わせてみるか\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents[:10]).with_config({\"run_name\": \"bm25_retriever\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748be073",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = {\n",
    "    \"bm25_documents\":bm25_retriever,\n",
    "    \"chroma_documents\":retriever\n",
    "} | xxxxx | rerank_process\n",
    "# みたいな感じで、両方の retriever から出てきた Document を RRF に従って並べ替える\n",
    "# こんな感じで、最終的には 1つの retriever にまとめることができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20072d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
